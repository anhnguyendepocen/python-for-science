{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Lecture-9\" data-toc-modified-id=\"Lecture-9-1\">Lecture 9</a></span><ul class=\"toc-item\"><li><span><a href=\"#Statistics\" data-toc-modified-id=\"Statistics-1.1\">Statistics</a></span></li><li><span><a href=\"#11.1-Introduction-to-Statistics\" data-toc-modified-id=\"11.1-Introduction-to-Statistics-1.2\">11.1 Introduction to Statistics</a></span><ul class=\"toc-item\"><li><span><a href=\"#11.1.1-What-is-Statistics?\" data-toc-modified-id=\"11.1.1-What-is-Statistics?-1.2.1\">11.1.1 What is Statistics?</a></span></li><li><span><a href=\"#11.1.2-How-is-Statistics-used-in-Data-Science?\" data-toc-modified-id=\"11.1.2-How-is-Statistics-used-in-Data-Science?-1.2.2\">11.1.2 How is Statistics used in Data Science?</a></span></li><li><span><a href=\"#11.1.3-Statistics-in-Python:\" data-toc-modified-id=\"11.1.3-Statistics-in-Python:-1.2.3\">11.1.3 Statistics in Python:</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Example-1-(Importing-Statistics-Packages):\" data-toc-modified-id=\"Example-1-(Importing-Statistics-Packages):-1.2.3.0.1\">Example 1 (Importing Statistics Packages):</a></span></li><li><span><a href=\"#Example-2-(Testing-Statistics-Packages):\" data-toc-modified-id=\"Example-2-(Testing-Statistics-Packages):-1.2.3.0.2\">Example 2 (Testing Statistics Packages):</a></span></li></ul></li></ul></li><li><span><a href=\"#11.1.3-Types-of-Statistics:\" data-toc-modified-id=\"11.1.3-Types-of-Statistics:-1.2.4\">11.1.3 Types of Statistics:</a></span></li></ul></li><li><span><a href=\"#11.2-Data-and-Distributions:\" data-toc-modified-id=\"11.2-Data-and-Distributions:-1.3\">11.2 Data and Distributions:</a></span><ul class=\"toc-item\"><li><span><a href=\"#11.2.1-Types-of-Data:\" data-toc-modified-id=\"11.2.1-Types-of-Data:-1.3.1\">11.2.1 Types of Data:</a></span></li><li><span><a href=\"#11.2.1.1-Qualitative-Data:\" data-toc-modified-id=\"11.2.1.1-Qualitative-Data:-1.3.2\">11.2.1.1 Qualitative Data:</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Problem-1:\" data-toc-modified-id=\"Problem-1:-1.3.2.0.1\">Problem 1:</a></span></li></ul></li></ul></li><li><span><a href=\"#11.2.1.2-Quantitative-Data:\" data-toc-modified-id=\"11.2.1.2-Quantitative-Data:-1.3.3\">11.2.1.2 Quantitative Data:</a></span></li><li><span><a href=\"#11.2.2-Graphing-Data:\" data-toc-modified-id=\"11.2.2-Graphing-Data:-1.3.4\">11.2.2 Graphing Data:</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Example-1-(Graphing-Qualitative-Data):\" data-toc-modified-id=\"Example-1-(Graphing-Qualitative-Data):-1.3.4.0.1\">Example 1 (Graphing Qualitative Data):</a></span></li><li><span><a href=\"#Example-1.1-(Frequency-Table):\" data-toc-modified-id=\"Example-1.1-(Frequency-Table):-1.3.4.0.2\">Example 1.1 (Frequency Table):</a></span></li><li><span><a href=\"#Example-1.2-(Pie-Charts):\" data-toc-modified-id=\"Example-1.2-(Pie-Charts):-1.3.4.0.3\">Example 1.2 (Pie Charts):</a></span></li><li><span><a href=\"#Example-1.3-(Bar-Charts):\" data-toc-modified-id=\"Example-1.3-(Bar-Charts):-1.3.4.0.4\">Example 1.3 (Bar Charts):</a></span></li><li><span><a href=\"#Example-2-(Graphing-Quantitative-Data):\" data-toc-modified-id=\"Example-2-(Graphing-Quantitative-Data):-1.3.4.0.5\">Example 2 (Graphing Quantitative Data):</a></span></li><li><span><a href=\"#Example-2.1-(Histogram):\" data-toc-modified-id=\"Example-2.1-(Histogram):-1.3.4.0.6\">Example 2.1 (Histogram):</a></span></li><li><span><a href=\"#Example-2.2-(Scatter-Plots):\" data-toc-modified-id=\"Example-2.2-(Scatter-Plots):-1.3.4.0.7\">Example 2.2 (Scatter Plots):</a></span></li><li><span><a href=\"#Example-2.3-(Line-Graphs):\" data-toc-modified-id=\"Example-2.3-(Line-Graphs):-1.3.4.0.8\">Example 2.3 (Line Graphs):</a></span></li><li><span><a href=\"#Problem-2\" data-toc-modified-id=\"Problem-2-1.3.4.0.9\">Problem 2</a></span></li></ul></li></ul></li><li><span><a href=\"#11.2.3-Summarizing-Data-and-Distributions:\" data-toc-modified-id=\"11.2.3-Summarizing-Data-and-Distributions:-1.3.5\">11.2.3 Summarizing Data and Distributions:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Part-1-(Central-Tendency):\" data-toc-modified-id=\"Part-1-(Central-Tendency):-1.3.5.1\">Part 1 (Central Tendency):</a></span><ul class=\"toc-item\"><li><span><a href=\"#Example-1.1-(Mean-of-Data):\" data-toc-modified-id=\"Example-1.1-(Mean-of-Data):-1.3.5.1.1\">Example 1.1 (Mean of Data):</a></span></li><li><span><a href=\"#Example-1.2-(Median-of-Data):\" data-toc-modified-id=\"Example-1.2-(Median-of-Data):-1.3.5.1.2\">Example 1.2 (Median of Data):</a></span></li><li><span><a href=\"#Example-1.3-(Mode-of-Data):\" data-toc-modified-id=\"Example-1.3-(Mode-of-Data):-1.3.5.1.3\">Example 1.3 (Mode of Data):</a></span></li><li><span><a href=\"#Example-1.4-(Mean-of-Distribution)\" data-toc-modified-id=\"Example-1.4-(Mean-of-Distribution)-1.3.5.1.4\">Example 1.4 (Mean of Distribution)</a></span></li></ul></li><li><span><a href=\"#Part-2-(Dispersion):\" data-toc-modified-id=\"Part-2-(Dispersion):-1.3.5.2\">Part 2 (Dispersion):</a></span><ul class=\"toc-item\"><li><span><a href=\"#Example-2.1-(Variance-of-Data):\" data-toc-modified-id=\"Example-2.1-(Variance-of-Data):-1.3.5.2.1\">Example 2.1 (Variance of Data):</a></span></li><li><span><a href=\"#Example-2.2-(Standard-Deviation-of-Data):\" data-toc-modified-id=\"Example-2.2-(Standard-Deviation-of-Data):-1.3.5.2.2\">Example 2.2 (Standard Deviation of Data):</a></span></li><li><span><a href=\"#Example-2.3-(Variance-of-Distribution):\" data-toc-modified-id=\"Example-2.3-(Variance-of-Distribution):-1.3.5.2.3\">Example 2.3 (Variance of Distribution):</a></span></li></ul></li></ul></li><li><span><a href=\"#11.2.4-Theoretical-vs.-Empirical-Probability-Distributions:\" data-toc-modified-id=\"11.2.4-Theoretical-vs.-Empirical-Probability-Distributions:-1.3.6\">11.2.4 Theoretical vs. Empirical Probability Distributions:</a></span></li><li><span><a href=\"#11.2.5-Common-Theoretical-Probability-Distributions:\" data-toc-modified-id=\"11.2.5-Common-Theoretical-Probability-Distributions:-1.3.7\">11.2.5 Common Theoretical Probability Distributions:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Part-1-(Normal-Distribution):\" data-toc-modified-id=\"Part-1-(Normal-Distribution):-1.3.7.1\">Part 1 (Normal Distribution):</a></span><ul class=\"toc-item\"><li><span><a href=\"#Example-1.1-(Probability-Density-Function---mean-=-0-and-var-=-1):\" data-toc-modified-id=\"Example-1.1-(Probability-Density-Function---mean-=-0-and-var-=-1):-1.3.7.1.1\">Example 1.1 (Probability Density Function - mean = 0 and var = 1):</a></span></li><li><span><a href=\"#Example-1.2-(Probability-Density-Function---mean-=--2,0,2-and-var-=-1):\" data-toc-modified-id=\"Example-1.2-(Probability-Density-Function---mean-=--2,0,2-and-var-=-1):-1.3.7.1.2\">Example 1.2 (Probability Density Function - mean = -2,0,2 and var = 1):</a></span></li><li><span><a href=\"#Example-1.3-(Probability-Density-Function---mean-=-0-and-var-=-0.5,-1,-4):\" data-toc-modified-id=\"Example-1.3-(Probability-Density-Function---mean-=-0-and-var-=-0.5,-1,-4):-1.3.7.1.3\">Example 1.3 (Probability Density Function - mean = 0 and var = 0.5, 1, 4):</a></span></li><li><span><a href=\"#Example-1.4-(Cumulative-Distribution--Function---mean-=-0-and-var-=-1):\" data-toc-modified-id=\"Example-1.4-(Cumulative-Distribution--Function---mean-=-0-and-var-=-1):-1.3.7.1.4\">Example 1.4 (Cumulative Distribution  Function - mean = 0 and var = 1):</a></span></li></ul></li><li><span><a href=\"#Part-2-(Binomial-Distribution):\" data-toc-modified-id=\"Part-2-(Binomial-Distribution):-1.3.7.2\">Part 2 (Binomial Distribution):</a></span><ul class=\"toc-item\"><li><span><a href=\"#Example-2.1-(Probability-Mass-Function---n-=-100000-and-p-=-0.5):\" data-toc-modified-id=\"Example-2.1-(Probability-Mass-Function---n-=-100000-and-p-=-0.5):-1.3.7.2.1\">Example 2.1 (Probability Mass Function - n = 100000 and p = 0.5):</a></span></li><li><span><a href=\"#Example-2.2-(Cumulative-Distribution--Function---n-=-100000-and-p-=-0.5):\" data-toc-modified-id=\"Example-2.2-(Cumulative-Distribution--Function---n-=-100000-and-p-=-0.5):-1.3.7.2.2\">Example 2.2 (Cumulative Distribution  Function - n = 100000 and p = 0.5):</a></span></li></ul></li><li><span><a href=\"#Part-3-(Uniform-Distribution):\" data-toc-modified-id=\"Part-3-(Uniform-Distribution):-1.3.7.3\">Part 3 (Uniform Distribution):</a></span><ul class=\"toc-item\"><li><span><a href=\"#Example-3.1-(Probability-Density-Function---a-=-0-and-b-=-1):\" data-toc-modified-id=\"Example-3.1-(Probability-Density-Function---a-=-0-and-b-=-1):-1.3.7.3.1\">Example 3.1 (Probability Density Function - a = 0 and b = 1):</a></span></li><li><span><a href=\"#Example-3.2-(Cumulative-Distribution--Function---a-=-0-and-b-=-1):\" data-toc-modified-id=\"Example-3.2-(Cumulative-Distribution--Function---a-=-0-and-b-=-1):-1.3.7.3.2\">Example 3.2 (Cumulative Distribution  Function - a = 0 and b = 1):</a></span></li></ul></li><li><span><a href=\"#Part-4-(Exponential-Distribution):\" data-toc-modified-id=\"Part-4-(Exponential-Distribution):-1.3.7.4\">Part 4 (Exponential Distribution):</a></span><ul class=\"toc-item\"><li><span><a href=\"#Example-4.1-(Probability-Density-Function---beta-=-1/2-OR-lambda-=-2):\" data-toc-modified-id=\"Example-4.1-(Probability-Density-Function---beta-=-1/2-OR-lambda-=-2):-1.3.7.4.1\">Example 4.1 (Probability Density Function - beta = 1/2 OR lambda = 2):</a></span></li><li><span><a href=\"#Example-4.2-(Probability-Density-Function---beta-=-0.5,3,5-OR-lambda-=-2,-1/3,-1/5):\" data-toc-modified-id=\"Example-4.2-(Probability-Density-Function---beta-=-0.5,3,5-OR-lambda-=-2,-1/3,-1/5):-1.3.7.4.2\">Example 4.2 (Probability Density Function - beta = 0.5,3,5 OR lambda = 2, 1/3, 1/5):</a></span></li><li><span><a href=\"#Example-4.3-(Cumulative-Distribution--Function---beta-=-0.5-OR-lambda-=-2):\" data-toc-modified-id=\"Example-4.3-(Cumulative-Distribution--Function---beta-=-0.5-OR-lambda-=-2):-1.3.7.4.3\">Example 4.3 (Cumulative Distribution  Function - beta = 0.5 OR lambda = 2):</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#11.3-Bivariate-Data:\" data-toc-modified-id=\"11.3-Bivariate-Data:-1.4\">11.3 Bivariate Data:</a></span><ul class=\"toc-item\"><li><span><a href=\"#11.3.1-What-is-Bivariate-Data?\" data-toc-modified-id=\"11.3.1-What-is-Bivariate-Data?-1.4.1\">11.3.1 What is Bivariate Data?</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Example-1-(Graphing-FG-Attempts-vs.-Steals):\" data-toc-modified-id=\"Example-1-(Graphing-FG-Attempts-vs.-Steals):-1.4.1.0.1\">Example 1 (Graphing FG Attempts vs. Steals):</a></span></li><li><span><a href=\"#Example-2-(Graphing-Tm-Points-vs.-Attendance):\" data-toc-modified-id=\"Example-2-(Graphing-Tm-Points-vs.-Attendance):-1.4.1.0.2\">Example 2 (Graphing Tm Points vs. Attendance):</a></span></li></ul></li></ul></li><li><span><a href=\"#11.3.2-Correlation-and-Covariance:\" data-toc-modified-id=\"11.3.2-Correlation-and-Covariance:-1.4.2\">11.3.2 Correlation and Covariance:</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Example-1-(Covariance):\" data-toc-modified-id=\"Example-1-(Covariance):-1.4.2.0.1\">Example 1 (Covariance):</a></span></li><li><span><a href=\"#Example-2-(Correlation):\" data-toc-modified-id=\"Example-2-(Correlation):-1.4.2.0.2\">Example 2 (Correlation):</a></span></li><li><span><a href=\"#Problem-3\" data-toc-modified-id=\"Problem-3-1.4.2.0.3\">Problem 3</a></span></li><li><span><a href=\"#Example-3-(Correlation-Heat-Map):\" data-toc-modified-id=\"Example-3-(Correlation-Heat-Map):-1.4.2.0.4\">Example 3 (Correlation Heat Map):</a></span></li></ul></li></ul></li><li><span><a href=\"#11.3.3-Ancombe's-Quartet:\" data-toc-modified-id=\"11.3.3-Ancombe's-Quartet:-1.4.3\">11.3.3 Ancombe's Quartet:</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Example-1-(Mean-of-X-and-Y):\" data-toc-modified-id=\"Example-1-(Mean-of-X-and-Y):-1.4.3.0.1\">Example 1 (Mean of X and Y):</a></span></li><li><span><a href=\"#Example-2-(Variance-of-X-and-Y):\" data-toc-modified-id=\"Example-2-(Variance-of-X-and-Y):-1.4.3.0.2\">Example 2 (Variance of X and Y):</a></span></li><li><span><a href=\"#Example-3-(Correlation-between-X-and-Y):\" data-toc-modified-id=\"Example-3-(Correlation-between-X-and-Y):-1.4.3.0.3\">Example 3 (Correlation between X and Y):</a></span></li><li><span><a href=\"#Example-4-(Plotting):\" data-toc-modified-id=\"Example-4-(Plotting):-1.4.3.0.4\">Example 4 (Plotting):</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#11.4-Sample-vs.-Population\" data-toc-modified-id=\"11.4-Sample-vs.-Population-1.5\">11.4 Sample vs. Population</a></span><ul class=\"toc-item\"><li><span><a href=\"#11.4.1-What-is-a-Sample-and-Population?\" data-toc-modified-id=\"11.4.1-What-is-a-Sample-and-Population?-1.5.1\">11.4.1 What is a Sample and Population?</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Problem-4:\" data-toc-modified-id=\"Problem-4:-1.5.1.0.1\">Problem 4:</a></span></li></ul></li></ul></li><li><span><a href=\"#11.4.2-Sampling-Distributions:\" data-toc-modified-id=\"11.4.2-Sampling-Distributions:-1.5.2\">11.4.2 Sampling Distributions:</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Example-1-(Sampling-Distribution---from-Uniform-Distribution-with-sample-size-5-and-5-iterations):\" data-toc-modified-id=\"Example-1-(Sampling-Distribution---from-Uniform-Distribution-with-sample-size-5-and-5-iterations):-1.5.2.0.1\">Example 1 (Sampling Distribution - from Uniform Distribution with sample size 5 and 5 iterations):</a></span></li><li><span><a href=\"#Example-1.1-(Sampling-Distribution-of-the-Sample-Mean):\" data-toc-modified-id=\"Example-1.1-(Sampling-Distribution-of-the-Sample-Mean):-1.5.2.0.2\">Example 1.1 (Sampling Distribution of the Sample Mean):</a></span></li><li><span><a href=\"#Example-1.2-(Sampling-Distribution-of-the-Sample-Range):\" data-toc-modified-id=\"Example-1.2-(Sampling-Distribution-of-the-Sample-Range):-1.5.2.0.3\">Example 1.2 (Sampling Distribution of the Sample Range):</a></span></li><li><span><a href=\"#Example-1.3-(Sampling-Distribution-of-the-Sample-Median):\" data-toc-modified-id=\"Example-1.3-(Sampling-Distribution-of-the-Sample-Median):-1.5.2.0.4\">Example 1.3 (Sampling Distribution of the Sample Median):</a></span></li><li><span><a href=\"#Example-2-(Sampling-Distribution---from-Uniform-Distribution-with-sample-size-5-and-10000-iterations):\" data-toc-modified-id=\"Example-2-(Sampling-Distribution---from-Uniform-Distribution-with-sample-size-5-and-10000-iterations):-1.5.2.0.5\">Example 2 (Sampling Distribution - from Uniform Distribution with sample size 5 and 10000 iterations):</a></span></li><li><span><a href=\"#Example-2.1-(Sampling-Distribution-of-the-Sample-Mean):\" data-toc-modified-id=\"Example-2.1-(Sampling-Distribution-of-the-Sample-Mean):-1.5.2.0.6\">Example 2.1 (Sampling Distribution of the Sample Mean):</a></span></li><li><span><a href=\"#Example-2.2-(Sampling-Distribution-of-the-Sample-Range):\" data-toc-modified-id=\"Example-2.2-(Sampling-Distribution-of-the-Sample-Range):-1.5.2.0.7\">Example 2.2 (Sampling Distribution of the Sample Range):</a></span></li><li><span><a href=\"#Example-2.3-(Sampling-Distribution-of-the-Sample-Median):\" data-toc-modified-id=\"Example-2.3-(Sampling-Distribution-of-the-Sample-Median):-1.5.2.0.8\">Example 2.3 (Sampling Distribution of the Sample Median):</a></span></li><li><span><a href=\"#Example-3-(Sampling-Distribution---from-Uniform-Distribution-with-sample-size-25-and-10000-iterations):\" data-toc-modified-id=\"Example-3-(Sampling-Distribution---from-Uniform-Distribution-with-sample-size-25-and-10000-iterations):-1.5.2.0.9\">Example 3 (Sampling Distribution - from Uniform Distribution with sample size 25 and 10000 iterations):</a></span></li><li><span><a href=\"#Example-4-(Calculating-Standard-Error-of-the-Sample-Mean):\" data-toc-modified-id=\"Example-4-(Calculating-Standard-Error-of-the-Sample-Mean):-1.5.2.0.10\">Example 4 (Calculating Standard Error of the Sample Mean):</a></span></li></ul></li></ul></li><li><span><a href=\"#11.4.3-Central-Limit-Theorem:\" data-toc-modified-id=\"11.4.3-Central-Limit-Theorem:-1.5.3\">11.4.3 Central Limit Theorem:</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Example-1-(Central-Limit-Theorem---from-Exponential-Distribution):\" data-toc-modified-id=\"Example-1-(Central-Limit-Theorem---from-Exponential-Distribution):-1.5.3.0.1\">Example 1 (Central Limit Theorem - from Exponential Distribution):</a></span></li><li><span><a href=\"#Example-1.1-(Sample-size-of-5,-10000-iterations):\" data-toc-modified-id=\"Example-1.1-(Sample-size-of-5,-10000-iterations):-1.5.3.0.2\">Example 1.1 (Sample size of 5, 10000 iterations):</a></span></li><li><span><a href=\"#Example-1.2-(Sample-size-of-15,-10000-iterations):\" data-toc-modified-id=\"Example-1.2-(Sample-size-of-15,-10000-iterations):-1.5.3.0.3\">Example 1.2 (Sample size of 15, 10000 iterations):</a></span></li><li><span><a href=\"#Example-1.3-(Sample-size-of-30,-10000-iterations):\" data-toc-modified-id=\"Example-1.3-(Sample-size-of-30,-10000-iterations):-1.5.3.0.4\">Example 1.3 (Sample size of 30, 10000 iterations):</a></span></li><li><span><a href=\"#Example-1.3-(Sample-size-of-100,-10000-iterations):\" data-toc-modified-id=\"Example-1.3-(Sample-size-of-100,-10000-iterations):-1.5.3.0.5\">Example 1.3 (Sample size of 100, 10000 iterations):</a></span></li></ul></li></ul></li><li><span><a href=\"#11.4.4-Law-of-Large-Numbers:\" data-toc-modified-id=\"11.4.4-Law-of-Large-Numbers:-1.5.4\">11.4.4 Law of Large Numbers:</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Example-1-(Coin-Toss):\" data-toc-modified-id=\"Example-1-(Coin-Toss):-1.5.4.0.1\">Example 1 (Coin Toss):</a></span></li><li><span><a href=\"#Example-2-(Die-Roll):\" data-toc-modified-id=\"Example-2-(Die-Roll):-1.5.4.0.2\">Example 2 (Die Roll):</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#11.5-Hypothesis-Testing\" data-toc-modified-id=\"11.5-Hypothesis-Testing-1.6\">11.5 Hypothesis Testing</a></span><ul class=\"toc-item\"><li><span><a href=\"#11.5.1-What-is-Hypothesis-Testing?\" data-toc-modified-id=\"11.5.1-What-is-Hypothesis-Testing?-1.6.1\">11.5.1 What is Hypothesis Testing?</a></span></li><li><span><a href=\"#11.5.2-Outcomes-of-Hypothesis-Testing:\" data-toc-modified-id=\"11.5.2-Outcomes-of-Hypothesis-Testing:-1.6.2\">11.5.2 Outcomes of Hypothesis Testing:</a></span></li><li><span><a href=\"#11.5.3-Performing-Hypothesis-Tests:\" data-toc-modified-id=\"11.5.3-Performing-Hypothesis-Tests:-1.6.3\">11.5.3 Performing Hypothesis Tests:</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Example-1-(One-Sided-Hypothesis-Test):\" data-toc-modified-id=\"Example-1-(One-Sided-Hypothesis-Test):-1.6.3.0.1\">Example 1 (One-Sided Hypothesis Test):</a></span></li><li><span><a href=\"#Problem-5:\" data-toc-modified-id=\"Problem-5:-1.6.3.0.2\">Problem 5:</a></span></li></ul></li></ul></li><li><span><a href=\"#ANSWERS-TO-LECTURE-11-PROBLEMS:\" data-toc-modified-id=\"ANSWERS-TO-LECTURE-11-PROBLEMS:-1.6.4\">ANSWERS TO LECTURE 11 PROBLEMS:</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Problem-1:\" data-toc-modified-id=\"Problem-1:-1.6.4.0.1\">Problem 1:</a></span></li><li><span><a href=\"#Problem-2\" data-toc-modified-id=\"Problem-2-1.6.4.0.2\">Problem 2</a></span></li></ul></li><li><span><a href=\"#Part-1---Qualitative-Variables:\" data-toc-modified-id=\"Part-1---Qualitative-Variables:-1.6.4.1\">Part 1 - Qualitative Variables:</a></span></li><li><span><a href=\"#Part-2---Quantitative-Variables:\" data-toc-modified-id=\"Part-2---Quantitative-Variables:-1.6.4.2\">Part 2 - Quantitative Variables:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problem-3\" data-toc-modified-id=\"Problem-3-1.6.4.2.1\">Problem 3</a></span></li></ul></li><li><span><a href=\"#Part-1---Covariance-Matrix\" data-toc-modified-id=\"Part-1---Covariance-Matrix-1.6.4.3\">Part 1 - Covariance Matrix</a></span></li><li><span><a href=\"#Part-2---Correlation-Matrix\" data-toc-modified-id=\"Part-2---Correlation-Matrix-1.6.4.4\">Part 2 - Correlation Matrix</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problem-4:\" data-toc-modified-id=\"Problem-4:-1.6.4.4.1\">Problem 4:</a></span></li><li><span><a href=\"#Problem-5:\" data-toc-modified-id=\"Problem-5:-1.6.4.4.2\">Problem 5:</a></span></li></ul></li><li><span><a href=\"#Part-1---Manually:\" data-toc-modified-id=\"Part-1---Manually:-1.6.4.5\">Part 1 - Manually:</a></span></li><li><span><a href=\"#Part-2---Programmatically:\" data-toc-modified-id=\"Part-2---Programmatically:-1.6.4.6\">Part 2 - Programmatically:</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 9\n",
    "## Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Purpose:__ The purpose of this lecture is to offer a brief overview of the field of Statistics which is paramount to performing Data Science. We will cover the most important introductory topics of Statistics starting with the type of statistics, graphing and summarizing data, comparing sample vs. population, and concepts such as Central Limit Theorem, estimators, Law of Large Numbers, and then end with confidence intervals and hypothesis testing. \n",
    "\n",
    "__At the end of this lecture you will be able to:__\n",
    "> 1. Understand and define Statistics as well as differentiate between Decriptive and Inferential Statistics \n",
    "> 2. Graph and summarize data using measures of Central Tendency and Variation \n",
    "> 3. Understand the characteristics of some common distributions such as Normal, Binomial, Uniform, and Exponential \n",
    "> 4. Work with bi-variate data and calculate summary statistics such as Correlation and Causation \n",
    "> 5. Understand the differences between working with samples and populations \n",
    "> 6. Define the Central Limit Theoreom and Law of Large Numbers \n",
    "> 7. Perform Hypothesis Testing \n",
    "> 8. Calculate Confidence Intervals "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Introduction to Statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.1 What is Statistics? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__ \n",
    "\n",
    "- __[Statistics](https://en.wikipedia.org/wiki/Statistics):__ Statistics is a branch of Mathematics that deals with collecting, organizing, and interpreting data \n",
    "- Statistics is used in many fields from Social Science, Finance, Heathcare, and Sports \n",
    "- The goal of Statistics is to study a population and observe their characteristics. However, it is often difficult to obtain an entire population. Instead, we deal with a subset of the population known as a sample\n",
    "- Examples of areas you encounter in your day-to-day life which Statistics play a large role in: \n",
    "> 1. Census Data \n",
    "> 2. Sports Boxscores \n",
    "> 3. Weather Forecasts \n",
    "> 4. Political Campaigning \n",
    "> 5. Stock Market "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.2 How is Statistics used in Data Science? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__ \n",
    "- Statistics is one of main disciplines that make up the field of Data Science\n",
    "- Statistics provides tools for Data Scientistis such as: \n",
    "> 1. Determining how much of your result can be attributed to \"Signal\" and how much can be attributed to \"Noise\"\n",
    "> 2. Analyzing the efficacy of a data set and its collection methods before analysis is performed on the data\n",
    "> 3. Summarizing a data set in terms of descriptive statistics as well as plots and other metrics \n",
    "> 4. Testing a hypothesis one may have about the data (i.e. one variable influences the other, two groups are identical, etc.) \n",
    "> 5. Characterize data into one of the common distributions and then use this for prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.3 Statistics in Python: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__ \n",
    "- Python has a wide range of useful functions to perform Statistical routines. These functions are found in the following two Modules: \n",
    "> 1. __[`scipy.stats`](https://docs.scipy.org/doc/scipy-0.18.1/reference/stats.html):__ The `stats` module in the SciPy Package offers many Statistical functions such as mean, zscore, correlation as well as other sub-modules for Continuous Distributions, Multivariate Distributions, and Discrete Distributions\n",
    "> 2. __[`numpy`](https://docs.scipy.org/doc/numpy-1.14.0/reference/routines.statistics.html):__ The `numpy` package itself offers many Statistical Functions such as Order Statistics, Averages and Variances, Correlating, and Histograms\n",
    "\n",
    "__Helpful Points:__\n",
    "1. It is not necessary to access a sub-package within the `numpy` package like we did with `numpy.linalg`. Instead, we can simply execute the function directly from NumPy: `np.func_name`\n",
    "\n",
    "__Practice:__ Import the Statistics Modules in Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1 (Importing Statistics Packages):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import math \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2 (Testing Statistics Packages):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.describe([1,2,3,4,5]) # using scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([1,2,3,4,5]) # using numpy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1.3 Types of Statistics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__ \n",
    "- There are two different types of statistics: \n",
    "> 1. __[Descriptive Statistics](https://en.wikipedia.org/wiki/Descriptive_statistics):__ The purpose of Descriptive Statistics is to provide a summary of the data and its properties \n",
    ">> - Descriptive Statistics can take the form of simple metrics known as summary statistics (i.e. number of observations, minimum value, maximum value, mean, variance, etc.) or they can take the form of visualizations (i.e. histograms, scatterplots, pie charts, box plots, etc.) \n",
    ">> - Examples of Descriptive Statistics:<br>\n",
    ">> >__a.__ 75th percentile of height of men in the United States<br> \n",
    ">> >__b.__ Mean Field Goal Percentage of a professional baskebtall player in the NBA<br> \n",
    ">> >__c.__ Median salary of Data Scientistis across every major metropolitan area in the United States <br>\n",
    "> 2. __[Inferential Statistics](https://en.wikipedia.org/wiki/Statistical_inference):__ The purpose of Inferential Statistics is to make inferences about a population using a subset of the population known as a sample \n",
    ">> - Inferential Statistics begin with a hypothesis about the population and then the sample is used to prove or disprove the hypoothesis, effectively inferring something about the population\n",
    ">> - Examples of Inferential Statistics: <br>\n",
    ">> >__a.__ Survey is sent out to 1000 residents in Chicago asking them about their political views. The survey designers are interested in knowing if Chicago residents in different areas have more or less conservative views. <br>\n",
    ">> >__b.__ A clinician develops a new drug to help patients relieve anxiety. The clinician collects a sample of individuals and gives half of them the new drug and the other half are given a placebo drug. \n",
    "\n",
    "__Helpful Points:__ \n",
    "1. It is important to understand that the purpose of Descriptive Statistics is to simply collect and record metrics and nothing more. It does NOT involve any generalization beyond the summary statistics \n",
    "2. In an experiment it is common to use both Descriptive AND Inferential Statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Data and Distributions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.1 Types of Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__ \n",
    "- In general, we can distinguish between two types of data:\n",
    "> 1. __[Qualitative Data](https://en.wikipedia.org/wiki/Qualitative_property):__ Qualitative Data contains discrete categories known as levels or categories. Qualitative Data can be further classified into four levels as described below:\n",
    "> 2. __[Quantitative Data](https://en.wikipedia.org/wiki/Quantitative_research):__ Quantitative Data is also known as Numerical Data because it contains numbers as counts or measurements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.1.1 Qualitative Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__\n",
    "- Qualitative Data can be further broken down into four categories: \n",
    "> 1. __[Nominal](https://en.wikipedia.org/wiki/Level_of_measurement#Nominal_level):__ Nominal (Categorical) data includes levels that can be differentiated only based on the names of the categories and not on an implicit ordering\n",
    ">> - Nominal data can be operated using $=$ and $\\neq$ and can be grouped into categories \n",
    ">> - For example, Nominal Data includes Gender, States of the United States of America, Eye Color, Marital Status, etc.\n",
    ">> - We CAN'T say one level is less than or greater than another \n",
    "> 2. __[Ordinal](https://en.wikipedia.org/wiki/Level_of_measurement#Ordinal_scale):__ Ordinal data includes levels that can be differentiated based on an implicit ordering, but the distances between the levels is unknown\n",
    ">> - Ordinal data can be operated using $>$ and $<$ and can be sorted \n",
    ">> - For example, Likert Scale (Like, Like Somewhat, Neutral, Dislike Somewhat, Dislike)\n",
    ">> - We CAN'T say level 1 is less than level 2 by a difference of 1 \n",
    "> 3. __[Interval](https://en.wikipedia.org/wiki/Level_of_measurement#Interval_scale):__ Interval data includes levels that can be differentiated based on an implicit ordering AND the distances between the levels is known, but the ratio between levels is unknown \n",
    ">> - Interval data can be operated using $+$ and $-$ and can be measured using a yardstick method \n",
    ">> - For example, Interval data includes temperature on the Celsius Scale, Date when measured from a specific start point, and location in Cartesian coordinates \n",
    ">> - We CAN'T say level 1 is twice as large as level 2 \n",
    "> 4. __[Ratio](https://en.wikipedia.org/wiki/Level_of_measurement#Ratio_scale):__ Ratio data includes levels that can be differentiated based on an implicit ordering AND the distances between the level is known AND the ratio between the levels is known because there is a unique and non-arbitrary zero value \n",
    ">> - Ratio data can be operated using $*$ and $/$ and can be measured using a ratio operation \n",
    ">> - For example, Ratio data includes temperature on the Kelvin Scale and most measurements in Physics (mass, energy, etc.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 1:\n",
    "\n",
    "Give an example of each type of qualitative data above (other than the examples that are mentioned in the definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your answer here \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.1.2 Quantitative Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__\n",
    "- Quantitative Data can be further broken intwo categories: \n",
    "> 1. __[Discrete Data](https://en.wikipedia.org/wiki/Continuous_or_discrete_variable#Discrete_variable):__ Discrete data can only take particular values over a range\n",
    ">> - For example, number of three point shots attempted, made, and missed\n",
    "> 2. __[Continuous Data](https://en.wikipedia.org/wiki/Continuous_or_discrete_variable#Continuous_variable):__ Continuous data are not restricted to defined values over a range, but instead can take on any value over a range \n",
    ">> - For example, height and weight \n",
    "\n",
    "__Helpful Points:__\n",
    "1. Technically, Categorical Data mentioned above is also an example of Discrete Data \n",
    "2. It is possible to group Continuous Data into distinct, discrete categories known as bins (i.e. 100 - 120 cm, 121 - 130 cm, 131 - 150 cm, > 150 cm, etc.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.2 Graphing Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__ \n",
    "- Depending on the type of data there exists different types of graphs that we can use to explore the properties of the data \n",
    "> 1. __Qualitative Data:__ Qualitative Data can be summarized using the following methods:\n",
    ">> a. Frequency Table <br>\n",
    ">> b. Pie Charts <br>\n",
    ">> c. Bar Charts \n",
    "> 2. __Quantitative Data:__ Quantitative Data can be summarized using the following methods: \n",
    ">> a. Histograms <br>\n",
    ">> b. Scatter Plots <br>\n",
    ">> c. Line Graph \n",
    "\n",
    "__Helpful Points:__\n",
    "1. There is lots of debate and best practices shared regarding the type of graph that should be chosen. Feel free to read about this online, but [here](https://blog.hubspot.com/marketing/types-of-graphs-for-data-visualization) is one such resource\n",
    "\n",
    "__Practice:__ Examples of Graphing Data in Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NBA data from Lecture 5 willl be used again here to perform Statistical Analysis on. Recall the background of the data and the meaning of the variables from Lecture 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data to analyze \n",
    "nba_df = pd.read_csv(\"NBA_GameLog_2010_2017.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# view the data \n",
    "nba_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to datetime\n",
    "nba_df['Date'] = pd.to_datetime(nba_df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make index the datetime column for easier indexing\n",
    "nba_df.set_index(\"Date\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1 (Graphing Qualitative Data):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to determine which columns in the NBA data set are qualitative variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nba_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the GameType, Team, Date, Opp, W.L and Referree columns refer to qualitative data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1.1 (Frequency Table):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tor_2016_2017 = nba_df.loc[(nba_df.loc[:, \"Season\"] == 2017) & (nba_df.loc[:, \"Team\"] == \"TOR\"), ]\n",
    "tor_2016_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tor_2016_2017.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table = pd.DataFrame(tor_2016_2017.loc[:, \"W.L\"].value_counts())\n",
    "freq_table.columns = [\"Frequency\"]\n",
    "freq_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table[\"RelativeFrequency\"] = freq_table[\"Frequency\"] / freq_table.Frequency.sum()\n",
    "freq_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1.2 (Pie Charts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [5,5])\n",
    "plt.pie(freq_table.RelativeFrequency, labels=[\"W\", \"L\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1.3 (Bar Charts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(freq_table.index,freq_table.RelativeFrequency);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2 (Graphing Quantitative Data):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2.1 (Histogram):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of the team's points\n",
    "plt.hist(tor_2016_2017.loc[:, \"Tm.Pts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# histogram of the team's FG Percentage\n",
    "plt.hist(tor_2016_2017.loc[:, \"Tm.FG_Perc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(tor_2016_2017.loc[:, \"Tm.FG_Perc\"],kde=False) # same as above but using seaborn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two histograms above may not look the same because the \"binning\" process which allocates the data into a pre-defined number of bins may be different, affecting the final visual of the histogram. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the histograms shown above, \n",
    "\n",
    "> 1. __X-Axis__: The X-Axis represnts the range of values that the Team Field Goal Percentage takes on\n",
    "> 2. __Y-Axis__: The Y-Axis represents the frequency count of the number of observations (games) that fall into each bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cumulative frequency histogram \n",
    "sns.distplot(tor_2016_2017.loc[:, \"Tm.FG_Perc\"], hist_kws={\"cumulative\":True},kde_kws={\"cumulative\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cumulative frequency histogram shown above, \n",
    "\n",
    "> 1. __X-Axis__: The X-Axis represnts the range of values that the Team Field Goal Percentage takes on\n",
    "> 2. __Y-Axis__: The Y-Axis represents the cumulative frequency count of the number of observations (games) that fall into each bin. We can interpret this in the following way: \n",
    ">> - In 20% of the games, the team had a Field Goal Percentage of 0.40 or less\n",
    ">> - In about 50% of the games, the team had a Field Goal Percentage of 0.45 or less\n",
    ">> - In about 80% of the games, the team had a Field Goal Percentage of 0.50 or less "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2.2 (Scatter Plots):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tor_2016_2017_home = tor_2016_2017.loc[(tor_2016_2017.loc[:, \"Home\"] == 1), ]\n",
    "tor_2016_2017_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of the home attendance \n",
    "plt.scatter(tor_2016_2017_home.index, tor_2016_2017_home.loc[:, \"Home.Attendance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scatter plot of the turnovers \n",
    "plt.scatter(tor_2016_2017.index, tor_2016_2017.loc[:, \"Tm.TOV\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine scatter plot and histogram of the turnovers \n",
    "sns.jointplot(tor_2016_2017.loc[:, \"G\"], tor_2016_2017.loc[:, \"Tm.TOV\"], stat_func=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2.3 (Line Graphs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot 2 series - team and opponent points\n",
    "plt.plot(tor_2016_2017.loc[:, \"Tm.Pts\"], linestyle = '--',linewidth = 3, c = 'b')\n",
    "plt.plot(tor_2016_2017.loc[:, \"Opp.Pts\"],linewidth = 5, c = 'r') \n",
    "plt.legend(['Tm.Pts','Opp.Pts'],shadow = True, loc = 0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 2 \n",
    "\n",
    "Load in the Seattle Home Price Data from Lecture 6 then choose 2-3 qualitative variables and 2-3 quantitative variables to summarize using the techniques explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.3 Summarizing Data and Distributions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__\n",
    "- It is possible to summarize data directly, but we typically create a Probability Distribution for the univariate (single variable) data that we are interested in exploring \n",
    "- Recall the definition of a Probability Distribution from Lecture 10 (Probability Distributions describes the probabilities of occurrences of possible outcomes) \n",
    "- We can use one of two types of summaries to characterize the data and distributions: \n",
    "> 1. __[Central Tendency](https://en.wikipedia.org/wiki/Central_tendency):__ Central Tendency is the central value of a Probability Distribution and can be measured in many different ways. The most common measures of Central Tendency include the following:\n",
    ">> a. __Arithmetic Mean:__  The Arithmetic Mean refers to the arithmetic average of a group values which is equal to dividing the total sum of all the values by the number of values <br> \n",
    ">> b. __Median:__ The Median refers to the middle value and is found by ordering the data and choosing the value in the middle (or the average of the two middle values if the data has an even number of values) <br>\n",
    ">> c. __Mode:__ The mode refers to the value that occurs most often in the data and is found first by calculating the frequency of each value in the data abd then finding the value that has the highest frequency \n",
    "> 2. __[Variability/Dispersion](https://en.wikipedia.org/wiki/Statistical_dispersion):__ Dispersion refers to the variability or spread of the distribution for each data point to the center of the distribution. The most common measures of Dispersion include the following: \n",
    ">> a. __Variance:__ The Variance is the average of the squared differences of each value and the mean of the data/distribution <br>\n",
    ">> b. __Standard Deviation:__ Standard Deviation is the square root of the Variance \n",
    "\n",
    "__Helpful Points:__ \n",
    "1. Based on the type of data mentioned in section 11.2.1, different measures of Central Tendency are used:\n",
    ">> a. __Nominal Data:__ The Central Tendency in Nominal Data is the Mode <br>\n",
    ">> b. __Ordinal Data:__ The Central Tendency in Ordinal Data is the Median <br>\n",
    ">> c. __Interval Data:__ The Central Tendency in Interval Data is the Mean<br>\n",
    ">> d. __Ratio Data:__ The Central Tendency in Ratio Data is the Geometric Mean \n",
    "\n",
    "__Practice:__ Examples of Summarizing Data in Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1 (Central Tendency):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1.1 (Mean of Data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tor_2016_2017.loc[:, [\"Tm.Pts\", \"Opp.Pts\", 'Tm.FGM', 'Tm.FGA', 'Tm.FG_Perc', 'Tm.3PM', 'Tm.3PA',\n",
    "       'Tm.3P_Perc', 'Tm.FTM', 'Tm.FTA', 'Tm.FT_Perc', 'Tm.ORB', 'Tm.TRB',\n",
    "       'Tm.AST', 'Tm.STL', 'Tm.BLK', 'Tm.TOV', 'Tm.PF', 'Home.Attendance']].mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1.2 (Median of Data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove scientific notation \n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(tor_2016_2017.loc[:, [\"Tm.Pts\", \"Opp.Pts\", 'Tm.FGM', 'Tm.FGA', 'Tm.FG_Perc', 'Tm.3PM', 'Tm.3PA',\n",
    "       'Tm.3P_Perc', 'Tm.FTM', 'Tm.FTA', 'Tm.FT_Perc', 'Tm.ORB', 'Tm.TRB',\n",
    "       'Tm.AST', 'Tm.STL', 'Tm.BLK', 'Tm.TOV', 'Tm.PF', 'Home.Attendance']], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1.3 (Mode of Data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats.mode(tor_2016_2017.loc[:, [\"Tm.Pts\", \"Opp.Pts\", 'Tm.FGM', 'Tm.FGA', 'Tm.FG_Perc', 'Tm.3PM', 'Tm.3PA',\n",
    "       'Tm.3P_Perc', 'Tm.FTM', 'Tm.FTA', 'Tm.FT_Perc', 'Tm.ORB', 'Tm.TRB',\n",
    "       'Tm.AST', 'Tm.STL', 'Tm.BLK', 'Tm.TOV', 'Tm.PF', 'Home.Attendance']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1.4 (Mean of Distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create a Probability Distribution for the variable \"Team Points\". This can be done manually by finding the relative frequency of each unique value in the variable \"Team Points\". It can also be done programmatically using the `seaborn` Package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "team_points = tor_2016_2017.loc[:, \"Tm.Pts\"].value_counts().index\n",
    "team_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_points_prob = tor_2016_2017.loc[:, \"Tm.Pts\"].value_counts() / tor_2016_2017.loc[:, \"Tm.Pts\"].value_counts().sum()\n",
    "team_points_prob.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# manual probability distribution (prob. mass function)\n",
    "plt.bar(team_points, team_points_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kernel density plot \n",
    "sns.set_style('whitegrid')\n",
    "sns.kdeplot(tor_2016_2017.loc[:, \"Tm.Pts\"], bw=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(tor_2016_2017.loc[:, \"Tm.Pts\"],kde=True,bins=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the formula for the Expected Value for a Discrete Random Variable: The sum of the product of the possible values of the Random Variable and the probability of each value occurring. We can use the dot product function for this calculation. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_mean = np.dot(team_points, team_points_prob)\n",
    "distr_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 (Dispersion):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2.1 (Variance of Data):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma$ = $\\sqrt{\\sigma^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tor_2016_2017.loc[:, [\"Tm.Pts\", \"Opp.Pts\", 'Tm.FGM', 'Tm.FGA', 'Tm.FG_Perc', 'Tm.3PM', 'Tm.3PA',\n",
    "       'Tm.3P_Perc', 'Tm.FTM', 'Tm.FTA', 'Tm.FT_Perc', 'Tm.ORB', 'Tm.TRB',\n",
    "       'Tm.AST', 'Tm.STL', 'Tm.BLK', 'Tm.TOV', 'Tm.PF', 'Home.Attendance']].var(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2.2 (Standard Deviation of Data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tor_2016_2017.loc[:, [\"Tm.Pts\", \"Opp.Pts\", 'Tm.FGM', 'Tm.FGA', 'Tm.FG_Perc', 'Tm.3PM', 'Tm.3PA',\n",
    "       'Tm.3P_Perc', 'Tm.FTM', 'Tm.FTA', 'Tm.FT_Perc', 'Tm.ORB', 'Tm.TRB',\n",
    "       'Tm.AST', 'Tm.STL', 'Tm.BLK', 'Tm.TOV', 'Tm.PF', 'Home.Attendance']].std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2.3 (Variance of Distribution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_var = np.dot((team_points - distr_mean)**2, team_points_prob)\n",
    "distr_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.4 Theoretical vs. Empirical Probability Distributions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__ \n",
    "-  Using the data above, we were able to plot Probability Distributions. These distributions are known as __Empirical Distributions__ since they are observed empirically through a Random Experiment \n",
    "- We can contrast __Empirical Distributions__ with __Theoretical Distributions__ which are \"special\" cases of Probability Distributions that have defined characteristics including: \n",
    "> 1. Parameters that define the Probability Density Function \n",
    "> 1. Formula for Probability Density Function \n",
    "> 2. Formula for Expected Value (Central Tendency) \n",
    "> 3. Formula for Variance (Dispersion) \n",
    "- One of the primary goals of Statistics is to characterize/assume a given set of univariate data follows one of the many Theoretical Probability Distributions. There are many consequences of assuming data fits one of the Probability Distributions (for example, assuming data fits a Theoretical Normal Distribution allows us to perform Linear Regression with the data) \n",
    "\n",
    "__Helpful Points:__\n",
    "1. In the real world, data does not fit the Theoretical Probability Distributions exactly, but they will be \"close enough.\" There are tests that you can use to test whether a series of data fits a specific Probability Distribution with some level of certainty "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2.5 Common Theoretical Probability Distributions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__ \n",
    "- We saw in Lecture 10 that there are many Probability Distributions that are used commonly in Statistical Analysis. However, we will highlight the four most common Distributions here: \n",
    "> 1. __[Normal Distribution](https://en.wikipedia.org/wiki/Normal_distribution):__ Normal Distribution (Gaussian Distribution) is the most common Probability Distribution and many sets of real world data follows this type of distribution. Another interesting fact about the Normal Distribution is that as data becomes larger and larger, their Probability Distribution will look more like the Normal Distribution. The characteristics of a Normal Distribution include: \n",
    ">> a. __Type:__ Normal Distribution is Continuous <br> \n",
    ">> b. __Parameters:__ Normal Distribution is characterized by two parameters: (1) Mean ($\\mu$) and (2) Variance ($\\sigma^2$)<br>\n",
    ">> c. __Probability Density Function:__ The PDF of a Normal Distribution is: <br> \n",
    "<center> $f(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^-{\\frac{(x - \\mu)^2}{2\\sigma^2}}$ </center> \n",
    ">> d. __Expected Value:__ $E[X] = \\mu$<br>\n",
    ">> e. __Variance:__ $V[X] = \\sigma^2$<br>\n",
    ">> f. __Examples:__ Height of men in the United States \n",
    "> 2. __[Binomial Distribution](https://en.wikipedia.org/wiki/Binomial_distribution):__ Binomial Distribution describes data that comes from repeated __[Bernoulli Trials](https://en.wikipedia.org/wiki/Bernoulli_trial)__ which are just experiments that can only take on a \"1\" or \"0\". In total, the Binomial Distribution describes the number of trials ($n$) where each trial has $p$ Probability of success. The characteristics of a Binomial Distribution include: \n",
    ">> a. __Type:__ Binomial Distribution is Discrete <br> \n",
    ">> b. __Parameters:__ Binomial Distribution is characterized by two parameters: (1) Number of Trials ($n$) and (2) Probability of Success ($p$)<br>\n",
    ">> c. __Probability Density Function:__ The PMF of a Binomial Distribution is: <br> \n",
    "<center> $n \\choose k$$p^k(1 - p)^{n-k}$ </center> \n",
    ">> d. __Expected Value:__ $E[X] = np$<br>\n",
    ">> e. __Variance:__ $V[X] = np(1 - p)$<br>\n",
    ">> f. __Examples:__ Tossing a coin 10 ($n$) times, where each coin toss is a Bernoulli trial and the coin has probability $p$ of landing on heads. \n",
    "> 3. __[Uniform Distribution](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)):__ Uniform Distribution describes data where each possible value has the same probability of occurring. The characteristics of a Uniform Distribution include: \n",
    ">> a. __Type:__ Uniform Distribution is Continuous <br> \n",
    ">> b. __Parameters:__ Uniform Distribution is characterized by two parameters: (1) Left Bound ($a$) and (2) Right Bound ($b$)<br>\n",
    ">> c. __Probability Density Function:__ The PDF of a Uniform Distribution is: <br> \n",
    "<center> $\\frac{1}{(b - a)}$ for $x \\in [a, b]$ and 0, otherwise</center>\n",
    ">> d. __Expected Value:__ $E[X] = \\frac{1}{2}(a + b)$<br>\n",
    ">> e. __Variance:__ $V[X] = \\frac{1}{12}(b - a)^2$<br>\n",
    ">> f. __Examples:__ Random sampling from a data set, where each data point as the same chance of being selected  \n",
    "> 4. __[Exponential Distribution](https://en.wikipedia.org/wiki/Exponential_distribution):__ Exponential Distribution typically describes the time in between events occurring. Exponential Distribution is considered \"memoryless\" and therefore the history of the distribution has no bearing on predicting what will happen next. The characteristics of an Exponential Distribution include: \n",
    ">> a. __Type:__ Exponential Distribution is Continuous <br> \n",
    ">> b. __Parameters:__ Exponential Distribution is characterized by one parameter: Rate ($\\lambda$)<br>\n",
    ">> c. __Probability Density Function:__ The PDF of an Exponential Distribution is: <br> \n",
    "<center> $\\lambda e^{-\\lambda x}$</center>\n",
    ">> d. __Expected Value:__ $E[X] = \\lambda^{-1}$<br>\n",
    ">> e. __Variance:__ $V[X] = \\lambda^{-2}$<br>\n",
    ">> f. __Examples:__ Time between visits to the Emergency Room in a hospital can be described by an Exponential Distribution. People may arrive to the Emergency Room according to a Poisson Process (not covered here), but the time between these patient arrivals is Exponentially Distributed. Moreover, if the average time between patients is 30 minutes and patients have come in during the morning hours at 9:00 am, 9:25 am, 9:26 am, 10:00 am, the next patient will still be expected to come in after 30 minutes since the distribution has no \"memory\" of the past. \n",
    "\n",
    "__Helpful Points:__ \n",
    "1. We can simulate a Random Variable based on any distribution which just means for any values of x, we plug into the probability density/mass function and observe the resulting probability \n",
    "2. Each of the Probability Distributions above have corresponding Cumulative Distribution Functions with associated properties, but these are not shown \n",
    "3. Each of the Theoretical Probability Distributions will be shown below \n",
    "\n",
    "__Practice:__ Examples of Common Theoretical Probability Distributions in Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1 (Normal Distribution):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1.1 (Probability Density Function - mean = 0 and var = 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters of the distribution \n",
    "mu = 0 \n",
    "sigma = 1\n",
    "# define the number of samples \n",
    "num_samples = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate normal distribution \n",
    "norm_data = np.random.normal(mu, sigma, num_samples)\n",
    "norm_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `np.random.normal()` function generates random numbers from a Normal Distribution with parameters equal to $\\mu = 0$ and $\\sigma = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data.mean() # almost 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data.var() # almost 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the probability distribution \n",
    "sns.distplot(norm_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a special case of a Normal Distribution called a __Standard Normal Distribution__ since it is centered at 0 with a dispersion of 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1.2 (Probability Density Function - mean = -2,0,2 and var = 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters of the distribution \n",
    "mu_1 = 0\n",
    "mu_2 = -2\n",
    "mu_3 = 2\n",
    "sigma = 1\n",
    "# define the number of samples \n",
    "num_samples = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate normal distribution \n",
    "norm_data_1 = np.random.normal(mu_1, sigma, num_samples)\n",
    "norm_data_2 = np.random.normal(mu_2, sigma, num_samples)\n",
    "norm_data_3 = np.random.normal(mu_3, sigma, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the probability distributions \n",
    "sns.distplot(norm_data_1)\n",
    "sns.distplot(norm_data_2)\n",
    "sns.distplot(norm_data_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by adjusting the mean of the distribution and keeping the variance constant, we are simply shifting the distribution right and left. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1.3 (Probability Density Function - mean = 0 and var = 0.5, 1, 4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters of the distribution \n",
    "mu = 0\n",
    "sigma_1 = 0.5\n",
    "sigma_2 = 1\n",
    "sigma_3 = 4\n",
    "# define the number of samples \n",
    "num_samples = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate normal distribution \n",
    "norm_data_1 = np.random.normal(mu, sigma_1, num_samples)\n",
    "norm_data_2 = np.random.normal(mu, sigma_2, num_samples)\n",
    "norm_data_3 = np.random.normal(mu, sigma_3, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the probability distributions \n",
    "sns.distplot(norm_data_1)\n",
    "sns.distplot(norm_data_2)\n",
    "sns.distplot(norm_data_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by adjusting the variance of the distribution and keeping the mean constant, we are simply shrinking and expanding the distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1.4 (Cumulative Distribution  Function - mean = 0 and var = 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters of the distribution \n",
    "mu = 0 \n",
    "sigma = 1\n",
    "# define the number of samples \n",
    "num_samples = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate normal distribution \n",
    "norm_data = np.random.normal(mu, sigma, num_samples)\n",
    "norm_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(norm_data, hist_kws={\"cumulative\":True},kde_kws={\"cumulative\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 (Binomial Distribution):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2.1 (Probability Mass Function - n = 100000 and p = 0.5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters of the distribution \n",
    "n = 100000 \n",
    "p = 0.5\n",
    "# define the number of samples \n",
    "num_samples = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate binomial distribution \n",
    "binom_data = np.random.binomial(n, p, num_samples)\n",
    "binom_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `np.random.binomial()` function generates random numbers from a Binomial Distribution with parameters equal to $n = 100000$ and $p = 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binom_data.mean() # np = (100000)(0.5) = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binom_data.var() # np(1-p) = ((100000)(0.5))(1 - 0.5) = roughly 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the probability distribution \n",
    "sns.distplot(binom_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how it practically looks normal. This is because we had a large number of trials and as the data becomes larger and larger, the distribution approaches normal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2.2 (Cumulative Distribution  Function - n = 100000 and p = 0.5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(binom_data, hist_kws={\"cumulative\":True},kde_kws={\"cumulative\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3 (Uniform Distribution):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 3.1 (Probability Density Function - a = 0 and b = 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters of the distribution \n",
    "a = 0 \n",
    "b = 1\n",
    "# define the number of samples \n",
    "num_samples = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate uniform distribution \n",
    "uniform_data = np.random.uniform(a, b, num_samples)\n",
    "uniform_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `np.random.uniform()` function generates random numbers from a Uniform Distribution with parameters equal to $a = 0$ and $b = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_data.mean() # (a + b)/2 = (0 + 1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_data.var() # (b - a)^2/12 = (1 - 0)^2/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the probability distribution \n",
    "sns.distplot(uniform_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 3.2 (Cumulative Distribution  Function - a = 0 and b = 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(uniform_data, hist_kws={\"cumulative\":True},kde_kws={\"cumulative\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4 (Exponential Distribution):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 4.1 (Probability Density Function - beta = 1/2 OR lambda = 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters of the distribution \n",
    "beta = 0.5 \n",
    "# define the number of samples \n",
    "num_samples = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate exponential distribution \n",
    "exp_data = np.random.exponential(scale = beta, size = num_samples)\n",
    "exp_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `np.random.exponential()` function generates random numbers from an Exponential Distribution with parameters equal to $\\beta = 0.5$ OR $\\lambda = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data.mean() # mu = beta = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_data.var() # var = beta^2 = 0.25 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the probability distribution \n",
    "sns.distplot(exp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 4.2 (Probability Density Function - beta = 0.5,3,5 OR lambda = 2, 1/3, 1/5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters of the distribution \n",
    "beta_1 = 3\n",
    "beta_2 = 5\n",
    "# define the number of samples \n",
    "num_samples = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate normal distribution \n",
    "exp_data_1 = np.random.exponential(scale = beta_1, size = num_samples)\n",
    "exp_data_2 = np.random.exponential(scale = beta_2, size = num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the probability distributions \n",
    "sns.distplot(exp_data_1)\n",
    "sns.distplot(exp_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 4.3 (Cumulative Distribution  Function - beta = 0.5 OR lambda = 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters of the distribution \n",
    "beta = 0.5 \n",
    "# define the number of samples \n",
    "num_samples = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate exponential distribution \n",
    "exp_data = np.random.exponential(scale = beta, size = num_samples)\n",
    "exp_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(exp_data, hist_kws={\"cumulative\":True},kde_kws={\"cumulative\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 Bivariate Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.1 What is Bivariate Data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__ \n",
    "- So far, all of the examples have been using __Univariate Data__ which is simply one series (i.e. depicting one variable) \n",
    "- However, it is more common to have multiple series of data that we are interested in the relationship between \n",
    "- __Bivariate Data:__ Bivariate Data consists of data from two variables and each value from each variable is paired up with the other variable\n",
    "> - The goal of Bivariate Data is to investigate the association or the relationship between the two variables. This can be accomplished by simply looking at a graph of the two variables or using statistics that are designed to measure the association (see next section) \n",
    "> - Examples of Bivariate Data from the NBA data set include Team's Field Goal Attempts and Team's Steals. We may be interested in knowing whether a team's steals is associated with the number of shots they attempts (i.e. steal the ball in their defensive end and go back the other way and make a shot) \n",
    "\n",
    "__Helpful Points:__\n",
    "1. In some cases, we name each of the two variables as:\n",
    "> __[Dependent Variable](https://en.wikipedia.org/wiki/Dependent_and_independent_variables#Statistics):__ The Dependent Variable represents the outcome or the variable that is being studied <br>\n",
    "> __[Independent Variable](https://en.wikipedia.org/wiki/Dependent_and_independent_variables#Statistics):__ The Independent Variable represents the inputs or potential reasons for the outcome\n",
    "2. The characterization of Bivariate Data into Dependent and Independent Variables is especially useful for Regression Analysis (not covered here) \n",
    "\n",
    "__Practice:__ Examples of Graphing Bivariate Data in Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1 (Graphing FG Attempts vs. Steals):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scatter plot of the FGM % and steals \n",
    "plt.scatter(nba_df.loc[:, \"Tm.FGM\"], nba_df.loc[:, \"Tm.STL\"])\n",
    "plt.xlabel(\"Tm.FGM (%)\")\n",
    "plt.ylabel(\"Tm.STL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2 (Graphing Tm Points vs. Attendance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scatter plot of the TM Points and Attendance \n",
    "plt.scatter(tor_2016_2017_home.loc[:, \"Tm.Pts\"], tor_2016_2017_home.loc[:, \"Home.Attendance\"])\n",
    "plt.xlabel(\"Tm.Pts\")\n",
    "plt.ylabel(\"Home.Attendance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.2 Correlation and Covariance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__ \n",
    "- In addition to analyzing Bivariate Data by plotting the two series on the x and y axis of a chart, we can also use a series of statistics to evaluate the relationship between the two variables:\n",
    "> 1. __[Covariance](https://en.wikipedia.org/wiki/Covariance):__ Covariance measures how the two variables vary with each other. Covariance investigates if the two variables tend to increase and decrease together or if one variable increases when the other decreases or vice versa or if the variables do not vary at all with each other (covariance of 0)\n",
    ">> - The formula for Covariance can be represented in terms of Expected Value and Variance: \n",
    "<center> $Cov(X, Y) = \\sigma_{XY} = E[XY] - E[X][Y] = E[(X - \\mu_{X})(Y - \\mu_{Y})]$ </center> \n",
    ">> - The formula for Covariance can also be represented in the following way:\n",
    "<center> $Cov(X, Y) = \\sigma_{XY} = \\frac{\\sum_{i=1}^{n} (x_{i} - \\mu_{x})(y_{i} - \\mu_{y})}{n}$ </center>\n",
    ">> - If two variables $X$ and $Y$ are independent (recall from Lecture 10 the definition of independence), then $Cov(X,Y) = 0$, but the opposite is not always true \n",
    ">> - The problem with Covariance is that it is not normalized and therefore difficult to determine if the magnitude of Covariance is considered strong or weak. This warrants a normalized measure of variable association of which we define Correlation below \n",
    "> 2. __[Correlation](https://en.wikipedia.org/wiki/Correlation_and_dependence):__ Correlation is very similar to Covariance but is scaled by the Standard Deviations of the two variables such that Correlation can only range between -1 and +1. \n",
    ">> - The formula for Correlation can be represented in the following way:\n",
    "<center> $Cor(X, Y) = r_{XY} = \\frac{\\frac{\\sum_{i=1}^{n} (x_{i} - \\mu_{x})(y_{i} - \\mu_{y})}{n}}{\\sigma_x \\sigma_y}$ </center> \n",
    ">> - We can now interpret the mangitude of the Correlation and conclude that if the Correlation is positive, the variables move in the same direction, if the Correlation is negative is negative, the two variables move in opposite direction, and if the Correlation is 0, the variables do move together in either direction \n",
    "\n",
    "__Helpful Points:__\n",
    "1. It is possible to create Covariance and Correlation Matrices which calculates the Covariance and Correlation, respectively, of a series of variables and summarizes this information into a symmetric matrix\n",
    "2. Typically, we calculate the Covariance and Correlation Matrices of a data set to observe all the paired covariance and correlation values for every pair of variables \n",
    "\n",
    "__Practice:__ Examples of Correlation and Covariance in Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1 (Covariance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset of nba data for cov and cor calculations \n",
    "nba_df_subset = nba_df.loc[:, ['Tm.Pts', 'Tm.FG_Perc', 'Tm.3P_Perc', 'Tm.FT_Perc', \n",
    "                               'Tm.TRB','Tm.AST', 'Tm.STL', 'Tm.BLK', 'Tm.TOV', 'Tm.PF']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nba_df_subset_cov = pd.DataFrame(np.cov(nba_df_subset.T)) # cov function calculates row based covariances by default, but we want column covariances\n",
    "nba_df_subset_cov.columns = ['Tm.Pts', 'Tm.FG_Perc', 'Tm.3P_Perc', 'Tm.FT_Perc', \n",
    "                               'Tm.TRB','Tm.AST', 'Tm.STL', 'Tm.BLK', 'Tm.TOV', 'Tm.PF']\n",
    "nba_df_subset_cov.index = ['Tm.Pts', 'Tm.FG_Perc', 'Tm.3P_Perc', 'Tm.FT_Perc', \n",
    "                               'Tm.TRB','Tm.AST', 'Tm.STL', 'Tm.BLK', 'Tm.TOV', 'Tm.PF']\n",
    "nba_df_subset_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the covariance between every pair of variables in the subsetted data set. For example:\n",
    "- The covariance between Team Points and Team Total Rebounds is 10.01\n",
    "- The covariance between Team 3 Point Shot Percentage and Team Steal sis -0.003\n",
    "\n",
    "Note:\n",
    "1. The matrix is symmetric so you only need to consider either the upper right triangle or the lower left triangle \n",
    "2. The diagonal elements represent the covariance of the variable onto itself which is just the variance of that variable (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# variance of each column which is the diagonal elements of the covariance matrix \n",
    "nba_df_subset.var(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2 (Correlation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nba_df_subset_corr = pd.DataFrame(np.corrcoef(nba_df_subset.T)) # corrcoef function calculates row based correlations by default, but we want column correlations\n",
    "nba_df_subset_corr.columns = ['Tm.Pts', 'Tm.FG_Perc', 'Tm.3P_Perc', 'Tm.FT_Perc', \n",
    "                               'Tm.TRB','Tm.AST', 'Tm.STL', 'Tm.BLK', 'Tm.TOV', 'Tm.PF']\n",
    "nba_df_subset_corr.index = ['Tm.Pts', 'Tm.FG_Perc', 'Tm.3P_Perc', 'Tm.FT_Perc', \n",
    "                               'Tm.TRB','Tm.AST', 'Tm.STL', 'Tm.BLK', 'Tm.TOV', 'Tm.PF']\n",
    "nba_df_subset_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the correlation between every pair of variables in the subsetted data set. For example:\n",
    "- The correlation between Team Points and Team Total Rebounds is 0.127\n",
    "- The correlation between Team 3 Point Shot Percentage and Team Steal is -0.01\n",
    "\n",
    "Note:\n",
    "1. The matrix is symmetric so you only need to consider either the upper right triangle or the lower left triangle \n",
    "2. The diagonal elements represent the correlation of the variable onto itself which is perfect correlation (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 3 \n",
    "\n",
    "Select the quantitative variables from the Seattle Home Price data and develop both a Covariance and Correlation Matrix as shown above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 3 (Correlation Heat Map):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(np.corrcoef(nba_df_subset.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3.3 Ancombe's Quartet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__\n",
    "- The first stage of any Statistical Analysis should be graphing the data to visualize the properties of the Bivariate Data\n",
    "- Without graphing data and relying solely on the numerical statistics, you may encounter errors that are best summarized in Ancombe's Quartet\n",
    "- __[Ancombe's Quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet):__ Ancombe's Quartet comprises of 4 datasets, each bivariate data (X and Y) and each containing 10 data points. It can easily be shown that each dataset has virtually identical numerical statistics but look drastically different when graphed \n",
    "\n",
    "__Helpful Points:__\n",
    "1. The 4 datasets were first developed by [Francis Anscombe](https://en.wikipedia.org/wiki/Frank_Anscombe) and it is not known exactly how they were created, although similar datasets have also been developed \n",
    "\n",
    "__Practice:__ Examples of Ancombe's Quarter in Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data to analyze \n",
    "anc_quart = pd.read_csv(\"ancombes_quartet.csv\")\n",
    "anc_quart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1 (Mean of X and Y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of x\n",
    "anc_quart.loc[:, [\"x_1\", \"x_2\", \"x_3\", \"x_4\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of y\n",
    "anc_quart.loc[:, [\"y_1\", \"y_2\", \"y_3\", \"y_4\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2 (Variance of X and Y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance of x\n",
    "anc_quart.loc[:, [\"x_1\", \"x_2\", \"x_3\", \"x_4\"]].var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance of y\n",
    "anc_quart.loc[:, [\"y_1\", \"y_2\", \"y_3\", \"y_4\"]].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 3 (Correlation between X and Y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(anc_quart.loc[:, [\"x_1\", \"y_1\"]].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(anc_quart.loc[:, [\"x_2\", \"y_2\"]].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(anc_quart.loc[:, [\"x_3\", \"y_3\"]].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(anc_quart.loc[:, [\"x_4\", \"y_4\"]].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you looked at these numerical statistics alone and stopped here, would you conclude they are all the same data? Probably!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 4 (Plotting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,5])\n",
    "\n",
    "plt.suptitle('Ancombes Quartert',fontsize = 16)\n",
    "\n",
    "plt.subplot(2,2,1) \n",
    "plt.scatter(anc_quart.x_1,anc_quart.y_1)\n",
    "plt.title('Dataset 1')\n",
    "plt.subplot(2,2,2) \n",
    "plt.scatter(anc_quart.x_2,anc_quart.y_2)\n",
    "plt.title('Dataset 2')\n",
    "plt.subplot(2,2,3) \n",
    "plt.scatter(anc_quart.x_3,anc_quart.y_3)\n",
    "plt.title('Dataset 3')\n",
    "plt.subplot(2,2,4) \n",
    "plt.scatter(anc_quart.x_4,anc_quart.y_4)\n",
    "plt.title('Dataset 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 Sample vs. Population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.1 What is a Sample and Population? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__ \n",
    "- It is important to differentiate between a Sample and Population in Statistical Analysis and also understand which of the two you are working with \n",
    "- __[Population](https://en.wikipedia.org/wiki/Statistical_population):__ Population refers to the complete set of observations/items/events that are of interest in an experiment\n",
    "> - A researcher may be interested in a statistic regarding the population (for example, the mean of the population)\n",
    "> - The population is defined by the researcher of the experiment ahead of time and is defined in a convenient fashion for their purposes \n",
    "> - Typically, the population is too large to study, thus warranting a smaller study that will allow the researcher to make inferences about the population \n",
    "> - The statistic of interest in the population is called a __Population Parameter__ (i.e. the Population Mean is represented as $\\mu$ and is usually Greek letters)\n",
    "- __[Sample](https://en.wikipedia.org/wiki/Sample_(statistics):__ Sample refers to a subset of data that is collected from a population \n",
    "> - The purpose of the sample is to make inferences about the population, thus a sample must be chosen in a way that is representative of the population \n",
    "> - The statistic of interest in the population is called a __Sample Statistic__ (i.e. the Sample Mean is represented as $\\bar x$)\n",
    "\n",
    "__Helpful Points:__\n",
    "1. There is an entire field of Statistical Analysis that deals with methods of creating a representative sample, but this is beyond the scope of this course \n",
    "2. A key part of Inferential Statistics is estimating how far Sample Statistics may vary from Population Parameters \n",
    "\n",
    "__Practice:__ Examples of Samples and Populations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 4:\n",
    "\n",
    "The Executive Director of Professional Development Courses at Metis is interested in how likely the students of the Basic Python and Math Course of the April 2018 cohort are to recommend the course to a friend. To measure this, the Executive Director provides a survey to the students and asks them how likely they are to reccomend the course and 15 of 30 students answer the survey. Based on the answers of the 15 students who completed the survey, the Executive Director will make his conclusion. What is the Sample and what is the Population?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Option A:__ Population = all students who have taken BPM at Metis. Sample = students of the BPM April 2018 cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Option B:__ Population = all students who take Professional Development courses at Metis. Sample = students of the BPM class (all cohorts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Option C:__ Population = all students in the BPM April 2018 cohort. Sample = 15 students of the BPM April 2018 cohort who answered the survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your answer here \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.2 Sampling Distributions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__ \n",
    "- __[Sampling Distribution](https://en.wikipedia.org/wiki/Sampling_distribution):__ Sampling Distribution is the Probability Distribution of a given statistic that is developed from repeatedly sampling from a population and recording the statistic of interest \n",
    "> - It is possible to have a Sampling Distribution for any statistic that we are interested in \n",
    "> - Sampling Distribution is developed in the following way: \n",
    ">> __Step 1:__ Decide on a statistic of the population you are interested in (for example, the population mean) <br> \n",
    ">> __Step 2:__ Sample n observations from the population and calculate the sample statistic (sample mean, in this case) of this sample of size n. Record this sample statistic <br>\n",
    ">> __Step 3:__ Continue executing step 2 for a large number of iterations (i.e. 10,000), which means we have 10,000 samples of size n which corresponds to 10,000 sample means <br>\n",
    ">> __Step 4:__ Plot the Probability Distribution of the 10,000 sample means and this becomes the Sampling Distribution of the Sample Mean \n",
    "\n",
    "__Helpful Points:__\n",
    "1. The example above was for the Sampling Distribution of the Sample Mean, but any statistic can be calculated for each sample. For example, the Sampling Distribution of the Sample Range, Median, etc. \n",
    "2. It is helpful to specify some terminology: \n",
    "> __Estimand:__ The Estimand is the statistic we are interested in (i.e. the mean) <br> \n",
    "> __Estimate:__ The Estimate is the sample statistic we calculate for every given sample (this is a number) <br> \n",
    "> __Estimator:__ The Estimator is the Random Variable that is described by the Probability Distribution known as the Sampling Distribution. Therefore, each Estimate (sample statistic calculated for 10,000 samples) are realizations of this Random Variable \n",
    "3. The Probability Distribution called the Sampling Distribution also has parameters that define its distribution. The two parameters are the mean and standard deviation (known as the __Standard Error__). When the statistic is the sample mean, we call it the __Standard Error of the Mean__ and the formula is $\\sigma_{\\bar x} = \\frac{\\sigma}{\\sqrt{n}}$\n",
    "\n",
    "__Practice:__ Examples of Sampling Distributions in Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1 (Sampling Distribution - from Uniform Distribution with sample size 5 and 5 iterations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate uniform distribution \n",
    "a = 2\n",
    "b = 10\n",
    "num_samples = 100000\n",
    "uniform_data = np.random.uniform(a, b, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_data.mean() # (a + b)/2 = (2 + 10)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_data.var() # (b - a)^2/12 = (10 - 2)^2/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the probability distribution \n",
    "sns.distplot(uniform_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider the Population to be a series of numbers ranging from 1 to 10 that follow a Uniform Distribution. We will now develop a Sampling Distribution for the Sample Mean based on this Population. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1:__ We will choose the statistic to be the Mean (estimand). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2:__ Sample 5 observations from the Population and calculate the Sample Mean every time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the data into a dataframe to use Pandas sample function \n",
    "uniform_data_df = pd.DataFrame(uniform_data)\n",
    "uniform_data_df.columns = [\"Population Values\"]\n",
    "uniform_data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "# sample 5 random values with replacement (each value has equal weight of getting chosen)\n",
    "sample_1 = uniform_data_df[\"Population Values\"].sample(n, replace = True)\n",
    "sample_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate sample mean \n",
    "sample_1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3:__ Repeat Step 2 for 5 iterations and save the sample mean every time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us write a simple function to generate a Sample Distribution for any size sample, any number of iterations and for mean, range, and median sample statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_distr(n, num_iter, typ):\n",
    "    sample_stats = []\n",
    "    for i in range(num_iter):\n",
    "        sample_i = uniform_data_df[\"Population Values\"].sample(n, replace = True)\n",
    "        print(\"Current Sample:\")\n",
    "        print(sample_i.values)\n",
    "        if typ == \"mean\":\n",
    "            sample_i_stat = sample_i.mean()\n",
    "        elif typ == \"range\":\n",
    "            sample_i_stat = sample_i.max() - sample_i.min()\n",
    "        elif typ == \"median\":\n",
    "            sample_i_stat = sample_i.median()\n",
    "        print(\"Current Sample Statistic for the \" + typ)\n",
    "        print(sample_i_stat)\n",
    "        print(\"\\n\")\n",
    "        sample_stats.append(sample_i_stat)\n",
    "    return sample_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1.1 (Sampling Distribution of the Sample Mean):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the function for for a sample size of 5, 5 iterations and calculate the sample mean for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distr_mean = sample_distr(5, 5, \"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1.2 (Sampling Distribution of the Sample Range):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the function for for a sample size of 5, 5 iterations and calculate the sample range for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distr_range = sample_distr(5, 5, \"range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1.3 (Sampling Distribution of the Sample Median):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the function for for a sample size of 5, 5 iterations and calculate the sample median for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distr_median = sample_distr(5, 5, \"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 4:__ Plot the Probability Distribution of the Sample Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(distr_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(distr_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(distr_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2 (Sampling Distribution - from Uniform Distribution with sample size 5 and 10000 iterations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-initialize the function, but remove the print statements\n",
    "def sample_distr(n, num_iter, typ):\n",
    "    sample_stats = []\n",
    "    for i in range(num_iter):\n",
    "        sample_i = uniform_data_df[\"Population Values\"].sample(n, replace = True)\n",
    "        if typ == \"mean\":\n",
    "            sample_i_stat = sample_i.mean()\n",
    "        elif typ == \"range\":\n",
    "            sample_i_stat = sample_i.max() - sample_i.min()\n",
    "        elif typ == \"median\":\n",
    "            sample_i_stat = sample_i.median()\n",
    "        sample_stats.append(sample_i_stat)\n",
    "    return sample_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2.1 (Sampling Distribution of the Sample Mean):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_mean = sample_distr(5, 10000, \"mean\") # for mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(distr_mean) # sampling distribution of the sample mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(distr_mean) # mean of the sampling distribution of the sample mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.var(distr_mean) # variance of the sampling distribution of the sample mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2.2 (Sampling Distribution of the Sample Range):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_range = sample_distr(5, 10000, \"range\") # for range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(distr_range) # sampling distribution of the sample range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(distr_range) # mean of the sampling distribution of the sample range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(distr_range) # variance of the sampling distribution of the sample range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2.3 (Sampling Distribution of the Sample Median):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_median= sample_distr(5, 10000, \"median\") # for median "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(distr_median) # sampling distribution of the sample median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(distr_median) # mean of the sampling distribution of the sample median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(distr_median) # variance of the sampling distribution of the sample median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 3 (Sampling Distribution - from Uniform Distribution with sample size 25 and 10000 iterations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_mean = sample_distr(25, 10000, \"mean\") # for mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(distr_mean) # sampling distribution of the sample mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(distr_mean) # mean of the sampling distribution of the sample mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.var(distr_mean) # variance of the sampling distribution of the sample mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how when we increase the size of our sample size, the distribution becomes closer to normal, the mean of the Sampling Distribution approaches the population mean and the variance of the Sampling Distribution significantly decresaes. This is because if our sample size that we take from the population is larger, it is more likely that the sample contains the true population mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 4 (Calculating Standard Error of the Sample Mean):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know the standard deviation of the original distribution and the sample size, we can determine the standard deviation of the sampling distribution. This standard deviation in the case of the sampling distribution of the sample mean is called the Standard Error. \n",
    "\n",
    "<center> $\\sigma_{\\bar x} = \\frac {\\sigma}{\\sqrt{n}}$ </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate standard error of the sampling distribution above in example 3\n",
    "std_error_form = uniform_data.std() / math.sqrt(25)\n",
    "std_error_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(distr_mean) # same as above "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, there is a relationship between the sample size and the standard deviation of the sampling distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.3 Central Limit Theorem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__ \n",
    "- __[Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem):__ The Central Limit Theorem is a fundamental concept in Statistics and states that despite the original Probability Distribution, if you repeatedly sample from this distribution with sample size n (as n approaches infinity) and then plot the distribution of these sample means, your distribution will approach a Normal Distribution \n",
    "> - You can also take the sample sums and then plot the sample sums and the sampling distribution will approach a Normal Distribution\n",
    "> - As the sample size (n) grows, the distribution of the sample means will approach a more \"perfect\" Normal Distribution \n",
    "\n",
    "__Helpful Points:__\n",
    "1. We actually observed the Central Limit Theorem in the example above where the original distriution was Uniform and after plotting the sample means of 10,000 samples, the distribution looks very normal. This fact is true for any original distribution \n",
    "2. Although technically the \"perfect\" Normal Distribution is only achieved as n approaches infinity, it is sufficient to have a sample size of 5, 10, 25, etc. to achieve an \"almost perfect\" Normal Distribution \n",
    "\n",
    "__Practice:__ Examples of Central Limit Theorem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1 (Central Limit Theorem - from Exponential Distribution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate exponential distribution \n",
    "beta = 2\n",
    "num_samples = 100000\n",
    "exp_data = np.random.exponential(scale = beta, size = num_samples)\n",
    "exp_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the data into a dataframe to use Pandas sample function \n",
    "exponential_data_df = pd.DataFrame(exp_data)\n",
    "exponential_data_df.columns = [\"Population Values\"]\n",
    "exponential_data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data.mean() # mu = beta = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_data.var() # var = beta^2 = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the probability distribution \n",
    "sns.distplot(exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-initialize the function, but add in the sum and also allow any data to be passed in \n",
    "def sample_distr(n, num_iter, typ, dat):\n",
    "    sample_stats = []\n",
    "    for i in range(num_iter):\n",
    "        sample_i = dat.sample(n, replace = True)\n",
    "        if typ == \"mean\":\n",
    "            sample_i_stat = sample_i.mean()\n",
    "        elif typ == \"range\":\n",
    "            sample_i_stat = sample_i.max() - sample_i.min()\n",
    "        elif typ == \"median\":\n",
    "            sample_i_stat = sample_i.median()\n",
    "        elif typ == \"sum\":\n",
    "            sample_i_stat = sample_i.sum()\n",
    "        sample_stats.append(sample_i_stat)\n",
    "    return sample_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1.1 (Sample size of 5, 10000 iterations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_mean = sample_distr(5, 10000, \"mean\", exponential_data_df) # for mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(distr_mean) # sampling distribution of the sample mean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that a sample size of 5 is not large enough to achieve a Normal Distribution of the Sample Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1.2 (Sample size of 15, 10000 iterations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_mean = sample_distr(15, 10000, \"mean\", exponential_data_df) # for mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(distr_mean) # sampling distribution of the sample mean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the distribution is approaching a Normal Distribution, but still quite not there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1.3 (Sample size of 30, 10000 iterations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_mean = sample_distr(30, 10000, \"mean\", exponential_data_df) # for mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(distr_mean) # sampling distribution of the sample mean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the distribution is pretty close to a Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1.3 (Sample size of 100, 10000 iterations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_mean = sample_distr(100, 10000, \"mean\", exponential_data_df) # for mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(distr_mean) # sampling distribution of the sample mean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the distribution is now almost a \"perfect\" Normal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_sum = sample_distr(100, 10000, \"sum\", exponential_data_df) # for sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(distr_sum) # sampling distribution of the sample sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This property clearly works for the sample sum as well. This probability distribution shows the sampling distribution of the sample sums. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4.4 Law of Large Numbers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__ \n",
    "- __[Law of Large Numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers):__ The Law of Large Numbers states that if you perform an experiment many, many times, the average of these results will approach the expected value of the distribution that was being sampled from or the population parameter \n",
    "> - For example, if you roll a die 5 times (n = 5) and average the numbers shown, record this number, then roll the die 10 times (n = 10) and average the numbers shown, record this number, etc. you will find that as you roll the die more times, the average of the numbers shown (as n approaches infinity), will approach the expected value (3.5 for a 6-sided die)\n",
    "\n",
    "__Helpful Points:__\n",
    "1. We can formulate the Law of Large Numbers in mathematical notation:\n",
    "If $X$ is a Random Variable with Expected Value $E[X]$, we can sample from its probability distribution with size n and average the values obtained: \n",
    "\n",
    "<center> $\\bar X_n = \\frac{x_1 + x_2 + ... + x_n}{n}$ </center> \n",
    "<center> $\\bar X_n \\Rightarrow E[X]$ (the sample mean will approach the Expected Value) </center>\n",
    "<center> $\\bar X_n \\Rightarrow \\mu$ (the sample mean will approach the Population Mean) </center>\n",
    "\n",
    "2. It should be intuitive that if the number of trials is large enough, you should obtain the expected value or population mean \n",
    "\n",
    "__Practice:__ Examples of the Law of Large Numbers in Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1 (Coin Toss):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let X be a Random Variable that denotes the number of tails after 1000 coin tosses, where the probability of tossing a tails is equal to 0.5. Therefore, the Expected Value is\n",
    "\n",
    "<center> $E[x] = np = (1000)(0.5) = 500$ </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function to perform an experiment n times, where each iteration involves tossing a coin 'num_tosses' number of times\n",
    "def coin_toss(n, num_tosses):\n",
    "    num_tails = []\n",
    "    for i in range(n):\n",
    "        flips = []\n",
    "        for j in range(num_tosses):\n",
    "            flip = random.randint(0, 1)\n",
    "            flips.append(flip)\n",
    "        num_tails.append(np.sum(flips))\n",
    "    \n",
    "    sample_mean = np.mean(num_tails)\n",
    "    return(sample_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_toss(10, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing the experiment 10 times gets us close to the expected value, but not quite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: will take a long time to run. Try with a smaller value of n (i.e. 100)\n",
    "res = []\n",
    "# collect the sample means for all number of trials from 1 to 1000\n",
    "for i in range(1,1000):\n",
    "    res.append(coin_toss(i, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(res)\n",
    "plt.xlabel(\"Number of trials\")\n",
    "plt.ylabel(\"Average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that as the number of trials increase, the sample average approaches the expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 2 (Die Roll):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let X be a Random Variable that denotes the value of a die roll after 1 toss, where the probability of tossing any given number is equal to 1/6. Therefore, the Expected Value is\n",
    "\n",
    "<center> $E[x] = (1)(1/6) + (2)(1/6) + (3)(1/6) + (4)(1/6) + (5)(1/6) + (6)(1/6) = 3.5$ </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function to perform an experiment n times, where each iteration involves rolling a die 1 time \n",
    "def die_roll(n, output = False):\n",
    "    rolls = []\n",
    "    for i in range(n):\n",
    "        roll = random.randint(1, 6)\n",
    "        rolls.append(roll)\n",
    "    \n",
    "    if output:\n",
    "        print(rolls)\n",
    "        \n",
    "    sample_mean = np.mean(rolls)\n",
    "    return(sample_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "die_roll(10, output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing the experiment 10 times gets us close to the expected value, but not quite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "# collect the sample means for all number of trials from 1 to 10000\n",
    "for i in range(1,10000):\n",
    "    res.append(die_roll(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(res)\n",
    "plt.xlabel(\"Number of trials\")\n",
    "plt.ylabel(\"Average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that as the number of trials increase, the sample average approaches the expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: \n",
    "\n",
    "- Beware of the __[Gambler's Fallacy](https://en.wikipedia.org/wiki/Gambler%27s_fallacy):__ which is the mistaken belief that if something happens more frequently at a certain time, then it is bound to happen less frequently in the future \n",
    "- It may be easy to think that the Law of Large Numbers is true because in the beginning of the coin tosses if there are many instances of heads, for example, then the later coin tosses will inevitable be tails, making the overall result approach the Expected Value. This is not the case  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.5 Hypothesis Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.1 What is Hypothesis Testing? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__\n",
    "- __[Hypothesis Testing](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing)__ The general idea of Hypothesis Testing is to test if some basic truth holds\n",
    "> - Some terminology:\n",
    ">> a.__Null Hypothesis:__ The Null Hypothesis is the basic truth that you would like to check if holds and is usually denoted by $H_0$<br> \n",
    ">> b. __Alternative Hypothesis:__ The Alternative Hypothesis is the opposite/converse/contradiction of the basic truth and is usually denoted by $H_a$<br> \n",
    ">> c. __One-Sided/One-Tailed Hypothesis Test:__ A one-sided/one-tailed hypothesis test is checking only if a value is less than a number OR a value is greater than a number, but not both. For example:\n",
    "<center> $H_0$ = the mean height of male men in the United States is equal to 160 cm ($H_0$ = 160 cm) </center>  \n",
    "<center> $H_a$ = the mean height of male men is greater than 160 cm ($H_a$ > 160 cm) </center> \n",
    ">> d. __Two-Sided/Two-Tailed Hypothesis Test:__ A two-sided/two-tailed hypothesis test checks if a value is less than a number AND checks if a value is greater than a number. For example:\n",
    "<center> $H_0$ = the mean height of male men in the United States is equal to 160 cm ($H_0$ = 160 cm) </center>  \n",
    "<center> $H_a$ = the mean height of male men is greater than 160 cm or the mean height of male men is less than 160 cm ($H_a$ 160 cm $<$ height of men $<$ 160 cm) </center> \n",
    "\n",
    "__Helpful Points:__ \n",
    "1. The goal of the Hypothesis Test is to see if we should reject the Null Hypothesis or fail to reject the Null Hypothesis\n",
    "2. Failing to reject the Null Hypothesis is NOT the same as accepting the Null Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.2 Outcomes of Hypothesis Testing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__ \n",
    "- Now that we have defined the Null and Alternative Hypotheses, we can consider the possible outcomes which are best summarized as 4 possible results in the following table:\n",
    "<img src=\"img36.png\",width=500,height=500>\n",
    "\n",
    "__Helpful Points:__ \n",
    "1. The Type-I Error is typically described by $\\alpha$ = P(Type-I error) = P(Reject $H_0 \\mid H_0$ is true)\n",
    "2. The Type-II Error is typically described by $\\beta$ = P(Type-II error) = P(Reject $H_a \\mid H_a$ is true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5.3 Performing Hypothesis Tests:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overview:__ \n",
    "- There exist robust statistical tests to determine if the Null Hypothesis should be rejected or fail to be rejected\n",
    "- Performing a Hypothests Test involves 3 decisions: \n",
    "> 1. What is the Null Hypothesis? \n",
    "> 2. Do you want to control the Type-I error rate ($\\alpha$) or the Type-II error rate ($\\beta$)\n",
    "> 3. At what level do you wish to set the selecte error rate (either $\\alpha$ or $\\beta$ depending on the choice made in Step 2\n",
    "\n",
    "__Helpful Points:__\n",
    "1. Typically we control the Type-I error rate ($\\alpha$) and it is common for a problem to state \"with 95% confidence\" or \"at the 5% level\" which both refer to the Type-I error rate \n",
    "\n",
    "__Practice:__ Examples of Performing Hypothesis Tests in Python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example 1 (One-Sided Hypothesis Test):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose a teacher takes a sample of their class' exam marks and wants to determine if the mean of the exam marks is greater than 80%, with $\\alpha = 0.05$. Assume that the class's exam marks are normally distributed so we are sampling from an underlying normal distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_marks = [85, 91, 75, 79, 80, 81, 83]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1:__ The Null Hypothesis should be stated as $H_0 = 80$ and the Alternative Hypothesis is $H_a > 80$ since this is a One-Tailed Test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2:__ Control the Type-I error rate $\\alpha$ at the 5% level "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3:__ Perform the Hypothesis Test which essentially involves calculating what the probability is observing a sample with the mean of 82 - did we observe this sample average by chance? \n",
    "\n",
    "__Step 3a:__ \n",
    "- We have to calculate a __critical region__ and then evaluate if the sample mean falls inside or outside the critical region\n",
    "- If the observed sample falls into the critical region, we would reject the Null Hypothesis with 1 - $\\alpha = 95%$ confidence \n",
    "- If the observed sample falls outside the critical region, we would fail to reject the Null Hypothesis \n",
    "\n",
    "__Step 3b:__ \n",
    "- To consruct a critical region, we must calculate a __test statistic__ which represents the sample. There are many test statistics, but we will use the following statistic:\n",
    "\n",
    "<center> $T = \\frac{\\bar X - 80}{\\sqrt{S^2/n}}$ </center> \n",
    "\n",
    "- $\\bar X$ = Estimates of the mean of X (distribution that the sample has been chosen from) \n",
    "- $S^2$ = Estimates of the variance of X (distribution that the sample has been chosen from) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_squared = (1/(len(exam_marks)-1))*np.sum((exam_marks - np.mean(exam_marks))**2) # unbiased sample variance \n",
    "test_stat = (np.mean(exam_marks) - 80)/(math.sqrt(S_squared / len(exam_marks))) # using formula above \n",
    "test_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the figure below to visualize where this test statistic lies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3c:__ \n",
    "- To determine if the test statistic lies inside or outside the critical region, we need to calculuate a __critical value__ corresponding to an alpha level of $\\alpha = 0.05$. We will then compare the test statistic with the critical value\n",
    "- The critical region is defined by the area in which the following holds:\n",
    "<center> $T \\geq t_{n-1}(\\alpha)$ </center>\n",
    "- The following states that the probability of the test statistic being greater than the critical value, given that the Null Hypothesis is true, is equal to alpha \n",
    "<center> $P(T \\geq t_{n-1}(\\alpha) \\mid H_0)$ </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_freedom = len(exam_marks) - 1 # degrees of freedom (parameter of t distribution)\n",
    "deg_freedom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critical_val = stats.t.ppf(0.95, deg_freedom) # inverse cdf \n",
    "critical_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the test statistic lies to the left of the critical value, the observed sample mean falls outside the critical region and we fail to reject the Null Hypothesis at the 5% significance level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the figure below to visualize where this t critical lies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3d:__\n",
    "- We can also calculate the probability of observing a sample mean greater than the sample mean observed\n",
    "- If this probability is less than the specified alpha level, then you must fail to reject the Null Hypothesis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "1 - stats.t.cdf(test_stat, deg_freedom) # probability to the right of the test statistic (must be less than 0.05 to reject H0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_1samp(exam_marks, 80) # one-sample t test (two-sided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.33651242446593654 / 2 # one-tailed probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a helpful figure to understand the above steps. We computed all the numbers in the figure manually above:\n",
    "<img src=\"img_37.png\",width=500,height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The above drawing is a rough sketch and is not drawn to scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 5:\n",
    "\n",
    "An experiment was run at the University of Chicago Medical School to test the efficacy of a new drug. Participants of the experiment were randmomly assigned into a \"test\" group and a \"control group\" where the first group was given the actual drug and the second group was given a placebo drug. The purpose of the drug was to help individuals concentrate when taking standardized tests. The following outlines the results of the standardized tests after taking the respective drugs:\n",
    "\n",
    "Group 1 (test): `[91, 76, 85, 88, 65, 90]`<br>\n",
    "Group 2 (control): `[75, 95, 76, 81, 80, 93]`\n",
    "\n",
    "Determine if the mean of the two groups are different at a confidence level of 95% by performing a Hypothesis Test according to the step-by-step method outlined above. Your answer must contain your Null Hypothesis, Alternative Hypothesis, test statistic, and critical value. You can check your answer by using the SciPy function `scipy.stats.ttest_ind`, but make sure you include a manual solution.\n",
    "\n",
    "NOTE: Since we have two samples now, we can't use the T Statistic mentioned above. Instead, use the following test statistic:\n",
    "\n",
    "<center> $T = \\frac{(\\bar x_{group1} - \\bar x_{group2}) - 0}{\\sqrt{S_1^2/n_1 + S_2^2/n_2}}$ </center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1 = [91, 76, 85, 88, 79, 90]\n",
    "group_2 = [75, 95, 76, 81, 80, 93]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(group_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(group_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWERS TO LECTURE 11 PROBLEMS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 1:\n",
    "\n",
    "Give an example of each type of categorical data above (other than the examples that are mentioned in the definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Nominal Variable:__ Political party (i.e. Democratic Party, Republican Party, etc.) <br>\n",
    "__Ordinal Variable:__ Socio-economic class (i.e. working class, middle class, upper-middle class, upper class) <br>\n",
    "__Interval Variable:__ Standardized Test Score (i.e. GRE, GMAT, SAT, etc.) <br>\n",
    "__Ratio Variable:__ Height, Weight, etc. (a height of zero means something significant, a weight of zero means something significant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 2 \n",
    "\n",
    "Load in the Seattle Home Price Data from Lecture 6 then choose 2-3 qualitative variables and 2-3 quantitative variables to summarize using the techniques explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_df = pd.read_csv(\"SeattleHomePrices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "home_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "home_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Choose the qualitative variable \"PROPERTY TYPE\"\n",
    "- Choose the quantitative variable \"PRICE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1 - Qualitative Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_table = pd.DataFrame(home_df.loc[:, \"PROPERTY TYPE\"].value_counts())\n",
    "freq_table.columns = [\"Frequency\"]\n",
    "freq_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq_table[\"RelativeFrequency\"] = freq_table[\"Frequency\"] / freq_table.Frequency.sum()\n",
    "freq_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = [5,5])\n",
    "plt.pie(freq_table.RelativeFrequency, labels=freq_table.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.bar(freq_table.index,freq_table.RelativeFrequency)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 - Quantitative Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(home_df.loc[:, \"PRICE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(home_df.index, home_df.loc[:, \"PRICE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = home_df.index\n",
    "y1 = home_df.loc[:, \"PRICE\"]\n",
    "y2 = home_df.loc[:, \"SQUARE FEET\"]\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(x, y1, 'g-')\n",
    "ax2.plot(x, y2, 'b-')\n",
    "\n",
    "ax1.set_xlabel('House Index')\n",
    "ax1.set_ylabel('Price', color='g')\n",
    "ax2.set_ylabel('Square Feet', color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 3 \n",
    "\n",
    "Select the quantitative variables from the Seattle Home Price data and develop both a Covariance and Correlation Matrix as shown above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_df_subset = home_df.loc[:, ['PRICE', 'BEDS', 'BATHS', 'SQUARE FEET', 'LOT SIZE',\n",
    "                               'YEAR BUILT', 'DAYS ON MARKET', '$/SQUARE FEET', 'HOA/MONTH']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_df_subset = home_df_subset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1 - Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_df_subset_cov = pd.DataFrame(np.cov(home_df_subset.T))\n",
    "home_df_subset_cov.columns = ['PRICE', 'BEDS', 'BATHS', 'SQUARE FEET', 'LOT SIZE',\n",
    "                               'YEAR BUILT', 'DAYS ON MARKET', '$/SQUARE FEET', 'HOA/MONTH']\n",
    "home_df_subset_cov.index = ['PRICE', 'BEDS', 'BATHS', 'SQUARE FEET', 'LOT SIZE',\n",
    "                               'YEAR BUILT', 'DAYS ON MARKET', '$/SQUARE FEET', 'HOA/MONTH']\n",
    "home_df_subset_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 - Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_df_subset_corr = pd.DataFrame(np.corrcoef(home_df_subset.T))\n",
    "home_df_subset_corr.columns = ['PRICE', 'BEDS', 'BATHS', 'SQUARE FEET', 'LOT SIZE',\n",
    "                               'YEAR BUILT', 'DAYS ON MARKET', '$/SQUARE FEET', 'HOA/MONTH']\n",
    "home_df_subset_corr.index = ['PRICE', 'BEDS', 'BATHS', 'SQUARE FEET', 'LOT SIZE',\n",
    "                               'YEAR BUILT', 'DAYS ON MARKET', '$/SQUARE FEET', 'HOA/MONTH']\n",
    "home_df_subset_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 4:\n",
    "\n",
    "The Executive Director of Professional Development Courses at Metis is interested in how likely the students of the Basic Python and Math Course of the April 2018 cohort are to recommend the course to a friend. To measure this, the Executive Director provides a survey to the students and asks them how likely they are to reccomend the course and 15 of 30 students answer the survey. Based on the answers of the 15 students who completed the survey, the Executive Director will make his conclusion. What is the Sample and what is the Population?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Option A:__ Population = all students who have taken BPM at Metis. Sample = students of the BPM April 2018 cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Option B:__ Population = all students who take Professional Development courses at Metis. Sample = students of the BPM class (all cohorts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Option C:__ Population = all students in the BPM April 2018 cohort. Sample = 15 students of the BPM April 2018 cohort who answered the survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is \"Option C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 5:\n",
    "\n",
    "An experiment was run at the University of Chicago Medical School to test the efficacy of a new drug. Participants of the experiment were randmomly assigned into a \"test\" group and a \"control group\" where the first group was given the actual drug and the second group was given a placebo drug. The purpose of the drug was to help individuals concentrate when taking standardized tests. The following outlines the results of the standardized tests after taking the respective drugs:\n",
    "\n",
    "Group 1 (test): `[91, 76, 85, 88, 65, 90]`<br>\n",
    "Group 2 (control): `[75, 95, 76, 81, 80, 93]`\n",
    "\n",
    "Determine if the mean of the two groups are different at a confidence level of 95% by performing a Hypothesis Test according to the step-by-step method outlined above. Your answer must contain your Null Hypothesis, Alternative Hypothesis, test statistic, and critical value. You can check your answer by using the SciPy function `scipy.stats.ttest_ind`, but make sure you include a manual solution.\n",
    "\n",
    "NOTE: Since we have two samples now, we can't use the T Statistic mentioned above. Instead, use the following test statistic:\n",
    "\n",
    "<center> $T = \\frac{(\\bar x_{group1} - \\bar x_{group2}) - 0}{\\sqrt{S_1^2/n_1 + S_2^2/n_2}}$ </center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1 = [91, 76, 85, 88, 79, 90]\n",
    "group_2 = [75, 95, 76, 81, 80, 93]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(group_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(group_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1 - Manually:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1:__ The Null Hypothesis should be stated as $H_0 : \\mu_{group1} = \\mu_{group2}$ which is the same as $H_0 : \\mu_{group1} - \\mu_{group2} = 0$ and the Alternative Hypothesis is $H_a : \\mu_{group1} \\neq \\mu_{group2}$ which is the same as $H_0 : \\mu_{group1} - \\mu_{group2} \\neq 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(group_1) - np.mean(group_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are essentially saying the Null Hypothesis is that the difference between the sample mean of group 1 and the sample mean of group 2 is equal to 0. Notice, we observe this sample difference to be 1.5, but the question is - is this number significantly different than 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2:__ Control the Type-I error rate $\\alpha$ at the 5% level "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3:__ Perform the Hypothesis Test which essentially involves calculating what the probability is observing a difference in sample mean equal to 1.5 - did we observe this difference in sample means by chance? \n",
    "\n",
    "__Step 3a:__ \n",
    "- We have to calculate a __critical region__ and then evaluate if the difference between the sample means falls inside or outside the critical region\n",
    "- If the observed difference in sample mean falls into the critical region, we would reject the Null Hypothesis with 1 - $\\alpha = 95%$ confidence \n",
    "- If the observed difference in sample mean outside the critical region, we would fail to reject the Null Hypothesis \n",
    "\n",
    "__Step 3b:__ \n",
    "- To consruct a critical region, we must calculate a __test statistic__ which represents the sample. There are many test statistics, but we will use the following statistic:\n",
    "\n",
    "<center> $T = \\frac{(\\bar x_{group1} - \\bar x_{group2}) - 0}{\\sqrt{S_1^2/n_1 + S_2^2/n_2}}$ </center> \n",
    "\n",
    "- $\\bar x_i$ = Estimates of the mean of group i  \n",
    "- $S_i^2$ = Estimates of the variance of group i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_squared_1 = (1/(len(group_1)-1))*np.sum((group_1 - np.mean(group_1))**2)\n",
    "S_squared_2 = (1/(len(group_2)-1))*np.sum((group_2 - np.mean(group_2))**2)\n",
    "test_stat = (np.mean(group_1) - np.mean(group_2))/(math.sqrt((S_squared_1 / len(group_1)) + (S_squared_2 / len(group_2))))  \n",
    "test_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3c:__ \n",
    "- To determine if the test statistic lies inside or outside the critical region, we need to calculuate a __critical value__ corresponding to an alpha level of $\\alpha = 0.05$. We will then compare the test statistic with the critical value\n",
    "- The critical region is defined by the area in which the following holds:\n",
    "<center> $T \\geq t_{n-1}(\\alpha)$ </center>\n",
    "- The following states that the probability of the test statistic being greater than the critical value, given that the Null Hypothesis is true, is equal to alpha \n",
    "<center> $P(T \\geq t_{n-1}(\\alpha) \\mid H_0)$ </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_freedom = len(group_1) + len(group_2) - 2\n",
    "deg_freedom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critical_val = stats.t.ppf(0.95, deg_freedom) # inverse cdf \n",
    "critical_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the test statistic lies to the left of the critical value, the observed sample mean falls outside the critical region and we fail to reject the Null Hypothesis at the 5% significance level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3d:__\n",
    "- We can also calculate the probability of observing a difference in sample means greater than the difference in sample means observed \n",
    "- If this probability is less than the specified alpha level, then you must fail to reject the Null Hypothesis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "1 - stats.t.cdf(test_stat, deg_freedom) # one-tailed probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1 - stats.t.cdf(test_stat, deg_freedom)) * 2 # two-tailed probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fail to reject the Null Hypothesis since this probability is greater than alpha "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 - Programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 sample t test in Python (assumes equal variance which is ok for now)\n",
    "stats.ttest_ind(group_1, group_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we fail to reject the Null Hypothesis that the two group mean values are equal. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
